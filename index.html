<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jonas Geiping</title> <meta name="author" content="Jonas Geiping"> <meta name="description" content="Webpage of Jonas Geiping. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jonas-geiping, machine-learning, safety, artificial-intelligence, security, privacy, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%AD&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jonasgeiping.github.io//"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Fira+Sans:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%6E%61%73.%67%65%69%70%69%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=206vNCEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/8284185" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/jonasgeiping" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/jonasgeiping" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About Me<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code</a> </li> <li class="nav-item "> <a class="nav-link" href="/openings/">Openings</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jonas</span> Geiping </h1> <p class="desc">Independent Research Group Leader <a href="https://institute-tue.ellis.eu/" rel="external nofollow noopener" target="_blank">ELLIS Institute</a> &amp; <a href="https://is.mpg.de/" rel="external nofollow noopener" target="_blank">MPI-IS</a></p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg?6ab01542d1195bab76f5bb1ea6fb2358" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> </div> </div> <div class="clearfix"> <p>Hi, I‚Äôm Jonas. I am starting as a research group leader in T√ºbingen, where I‚Äôm building a group for <strong>safety- &amp; efficiency- aligned learning</strong> (ü¶≠). Before this, I‚Äôve spent time at the University of Maryland and the University of Siegen.</p> <p>I am mostly interested in questions of safety and efficiency in modern machine learning. There are a number of fundamental machine learning questions that come up in these topics that we still do not understand well. In safety, examples are questions about the principles of data poisoning, the subtleties of water-marking for generative models, privacy questions in federated learning, or adversarial attacks against large language models. Can we ever make these models ‚Äúsafe‚Äù, and how do we define this? Are there feasible technical solutions that reduce harm?</p> <p>Further, I am interested in questions about the efficiency of modern AI systems, especially for large language models. How efficient can we make these systems, can we train strong models with little compute? Can we extend the capabilities of language models with recursive computation? How do efficiency modifications impact the safety of these models?</p> <p>In short:</p> <ul> <li>Safety, Security and Privacy in Machine Learning</li> <li>Efficient Machine Learning (especially for NLP)</li> <li>Trustworthy AI</li> <li>Deep Learning as-a-Science</li> </ul> <h3 id="incoming-phd-students">Incoming PhD Students:</h3> <p>If you are interested in these topics, feel free to reach out for more information! I‚Äôm currently hiring through the following PhD programs:</p> <ul> <li><a href="https://ellis.eu/phd-postdoc" rel="external nofollow noopener" target="_blank">ELLIS PhD program</a></li> <li><a href="https://learning-systems.org/" rel="external nofollow noopener" target="_blank">Max Planck &amp; ETH Center for Learning Systems (CLS)</a></li> <li><a href="https://imprs.is.mpg.de/" rel="external nofollow noopener" target="_blank">International Max Planck Research School for Intelligent Systems (IMPRS-IS)</a></li> </ul> <p>For more details, make sure to also check out the <a href="openings">openings</a> page.</p> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="fowl_decepticons_2023" class="col-sm-8"> <div class="title">Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models</div> <div class="author"> Liam H. Fowl,¬†<em>Jonas Geiping</em>,¬†Steven Reich,¬†<a href="https://yuxinwenrick.github.io/" rel="external nofollow noopener" target="_blank">Yuxin Wen</a>,¬†Wojciech Czaja,¬†<a href="https://goldblum.github.io/" rel="external nofollow noopener" target="_blank">Micah Goldblum</a>,¬†and¬†<a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="geiping_cramming_2023" class="col-sm-8"> <div class="title">Cramming: Training a Language Model on a Single GPU in One Day.</div> <div class="author"> <em>Jonas Geiping</em>,¬†and¬†<a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting. We provide code to reproduce all experiments at github.com/JonasGeiping/cramming .</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="jain_baseline_2023" class="col-sm-8"> <div class="title">Baseline Defenses for Adversarial Attacks Against Aligned Language Models</div> <div class="author"> <a href="https://neelsjain.github.io/" rel="external nofollow noopener" target="_blank">Neel Jain</a>,¬†<a href="https://www.cs.umd.edu/~avi1/" rel="external nofollow noopener" target="_blank">Avi Schwarzschild</a>,¬†<a href="https://yuxinwenrick.github.io/" rel="external nofollow noopener" target="_blank">Yuxin Wen</a>,¬†<a href="https://somepago.github.io/" rel="external nofollow noopener" target="_blank">Gowthami Somepalli</a>,¬†<a href="https://jwkirchenbauer.notion.site/" rel="external nofollow noopener" target="_blank">John Kirchenbauer</a>,¬†Ping-yeh Chiang,¬†<a href="https://goldblum.github.io/" rel="external nofollow noopener" target="_blank">Micah Goldblum</a>,¬†<a href="https://ani0075saha.github.io/" rel="external nofollow noopener" target="_blank">Aniruddha Saha</a>,¬†<em>Jonas Geiping</em>,¬†and¬†<a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>arxiv:2309.00614[cs]</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more success with filtering and preprocessing than we would expect from other domains, such as vision, providing a first indication that the relative strengths of these defenses may be weighed differently in these domains.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="kirchenbauer_watermark_2023" class="col-sm-8"> <div class="title">A Watermark for Large Language Models</div> <div class="author"> <a href="https://jwkirchenbauer.notion.site/" rel="external nofollow noopener" target="_blank">John Kirchenbauer</a>,¬†<em>Jonas Geiping</em>,¬†<a href="https://yuxinwenrick.github.io/" rel="external nofollow noopener" target="_blank">Yuxin Wen</a>,¬†Jonathan Katz,¬†Ian Miers,¬†and¬†<a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="somepalli_diffusion_2023" class="col-sm-8"> <div class="title">Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models</div> <div class="author"> <a href="https://somepago.github.io/" rel="external nofollow noopener" target="_blank">Gowthami Somepalli</a>,¬†<a href="https://vasusingla.github.io/" rel="external nofollow noopener" target="_blank">Vasu Singla</a>,¬†<a href="https://goldblum.github.io/" rel="external nofollow noopener" target="_blank">Micah Goldblum</a>,¬†<em>Jonas Geiping</em>,¬†and¬†<a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="wen_tree-ring_2023" class="col-sm-8"> <div class="title">Tree-Ring Watermarks: Fingerprints for Diffusion Images That Are Invisible and Robust</div> <div class="author"> <a href="https://yuxinwenrick.github.io/" rel="external nofollow noopener" target="_blank">Yuxin Wen</a>,¬†<a href="https://jwkirchenbauer.notion.site/" rel="external nofollow noopener" target="_blank">John Kirchenbauer</a>,¬†<em>Jonas Geiping</em>,¬†and¬†<a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>arxiv:2305.20030[cs]</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at github.com/YuxinWenRick/tree-ring-watermark.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%6F%6E%61%73.%67%65%69%70%69%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=206vNCEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/8284185" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/jonasgeiping" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/jonasgeiping" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> If you have research questions, feel free to reach out anytime via email or twitter! </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2023 Jonas Geiping. <a href="https://jonasgeiping.github.io//imprint/">Impressum</a>.Last updated: November 03, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>