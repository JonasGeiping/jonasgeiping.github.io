<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jonas Geiping</title> <meta name="author" content="Jonas Geiping"> <meta name="description" content="Webpage of Jonas Geiping. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jonas-geiping, geiping, machine-learning, safety, artificial-intelligence, security, privacy, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%AD&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jonasgeiping.github.io//"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Fira+Sans:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" rel="stylesheet"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%6E%61%73.%67%65%69%70%69%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=206vNCEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/8284185" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/jonasgeiping" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/jonasgeiping" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About Me<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/group/">Group</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code</a> </li> <li class="nav-item "> <a class="nav-link" href="/openings/">Openings</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jonas</span> Geiping </h1> <p class="desc">Research Group Leader <a href="https://institute-tue.ellis.eu/" rel="external nofollow noopener" target="_blank">ELLIS Institute</a> &amp; <a href="https://is.mpg.de/" rel="external nofollow noopener" target="_blank">Max-Planck Institute for Intelligent Systems</a> <br> <a href="https://tuebingen.ai/" rel="external nofollow noopener" target="_blank">Tübingen AI Center</a>, Germany</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg?e59266a0cae8adc688b2d23491a15a3c" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p>Tübingen, Germany</p> <p>ELLIS Institute</p> <p>Maria-von-Linden Straße 2</p> </div> </div> <div class="clearfix"> <p>Hi, I’m Jonas. I am a Machine Learning researcher in Tübingen, where I lead the research group for <strong>safety- &amp; efficiency- aligned learning</strong> (🦭). Before this, I’ve spent time at the Universities of Maryland, Siegen and Münster.</p> <p>I am constantly fascinated by questions of safety and efficiency in modern machine learning. There are a number of fundamental machine learning questions that come up in these topics that we still do not understand well. On the safety side, I investigate how models can be manipulated through data poisoning, jailbreaks, and adversarial attacks. I’m curious about watermarking for generative models, privacy guarantees in machine learning, and the challenge of defining “safety” in a meaningful technical way. Are there feasible technical solutions that reduce harm?</p> <p>For efficiency, I study how we can build systems that do more with less, from weight averaging techniques to recursive computation approaches that extend model capabilities. I’m particularly interested in how these systems reason, and whether we can enhance their reasoning abilities while maintaining efficiency. How do we build mechanisms that let these models learn to be intelligent systems? At the core of my research is this intersection: Can we make models that reason well without sacrificing safety? How do computational constraints affect safety guarantees? Can we design systems where intelligence and safety reinforce each other?</p> <p>In short:</p> <ul> <li>Safety, Security and Privacy in Machine Learning</li> <li>Understanding (and Implementing) Reasoning in Intelligent Systems</li> <li>Efficient Machine Learning (especially in Language Modeling)</li> <li>Deep Learning as-a-Natural-Science</li> </ul> <h3 id="incoming-phd-students">Incoming PhD Students:</h3> <p>If you are interested in these topics, feel free to reach out for more information! I am admitting a small number of PhD students through the following PhD programs:</p> <ul> <li><a href="https://ellis.eu/phd-postdoc" rel="external nofollow noopener" target="_blank">ELLIS PhD program</a></li> <li><a href="https://learning-systems.org/" rel="external nofollow noopener" target="_blank">Max Planck &amp; ETH Center for Learning Systems (CLS)</a></li> <li><a href="https://imprs.is.mpg.de/" rel="external nofollow noopener" target="_blank">International Max Planck Research School for Intelligent Systems (IMPRS-IS)</a></li> </ul> <p>For more details, make sure to read the <a href="openings">openings</a> page carefully.</p> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="geiping_scaling_2025" class="col-sm-8"> <div class="title">Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</div> <div class="author"> <em>Jonas Geiping</em>, Sean McLeish, <a href="https://neelsjain.github.io/" rel="external nofollow noopener" target="_blank">Neel Jain</a>, <a href="https://jwkirchenbauer.notion.site/" rel="external nofollow noopener" target="_blank">John Kirchenbauer</a>, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and <a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>arxiv:2502.05171[cs]</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="geiping_coercing_2024" class="col-sm-8"> <div class="title">Coercing LLMs to Do and Reveal (Almost) Anything</div> <div class="author"> <em>Jonas Geiping</em>, Alex Stein, Manli Shu, <a href="https://khalidsaifullaah.github.io/" rel="external nofollow noopener" target="_blank">Khalid Saifullah</a>, <a href="https://yuxinwenrick.github.io/" rel="external nofollow noopener" target="_blank">Yuxin Wen</a>, and <a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>arxiv:2402.14020[cs]</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="hans_spotting_2024" class="col-sm-8"> <div class="title">Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text</div> <div class="author"> <a href="https://ahans30.github.io/" rel="external nofollow noopener" target="_blank">Abhimanyu Hans</a>, <a href="https://www.cs.umd.edu/~avi1/" rel="external nofollow noopener" target="_blank">Avi Schwarzschild</a>, Valeriia Cherepanova, Hamid Kazemi, <a href="https://ani0075saha.github.io/" rel="external nofollow noopener" target="_blank">Aniruddha Saha</a>, <a href="https://goldblum.github.io/" rel="external nofollow noopener" target="_blank">Micah Goldblum</a>, <em>Jonas Geiping</em>, and <a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In Proceedings of the Forty-first International Conference on Machine Learning</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="mcleish_transformers_2024" class="col-sm-8"> <div class="title">Transformers Can Do Arithmetic with the Right Embeddings</div> <div class="author"> Sean Michael McLeish, Arpit Bansal, Alex Stein, <a href="https://neelsjain.github.io/" rel="external nofollow noopener" target="_blank">Neel Jain</a>, <a href="https://jwkirchenbauer.notion.site/" rel="external nofollow noopener" target="_blank">John Kirchenbauer</a>, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, <em>Jonas Geiping</em>, <a href="https://www.cs.umd.edu/~avi1/" rel="external nofollow noopener" target="_blank">Avi Schwarzschild</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Tom Goldstein' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further. With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="singh_democratizing_2024-1" class="col-sm-8"> <div class="title">Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers</div> <div class="author"> Siddharth Singh, Prajwal Singhania, Aditya Ranjan, <a href="https://jwkirchenbauer.notion.site/" rel="external nofollow noopener" target="_blank">John Kirchenbauer</a>, <em>Jonas Geiping</em>, <a href="https://yuxinwenrick.github.io/" rel="external nofollow noopener" target="_blank">Yuxin Wen</a>, <a href="https://neelsjain.github.io/" rel="external nofollow noopener" target="_blank">Neel Jain</a>, <a href="https://ahans30.github.io/" rel="external nofollow noopener" target="_blank">Abhimanyu Hans</a>, Manli Shu, Aditya Tomar, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tom Goldstein, Abhinav Bhatele' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 2024 SC24: International Conference for High Performance Computing, Networking, Storage and Analysis SC</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s). While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore “catastrophic memorization,” where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="geiping_cramming_2023" class="col-sm-8"> <div class="title">Cramming: Training a Language Model on a Single GPU in One Day.</div> <div class="author"> <em>Jonas Geiping</em>, and <a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting. We provide code to reproduce all experiments at github.com/JonasGeiping/cramming .</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="kirchenbauer_watermark_2023" class="col-sm-8"> <div class="title">A Watermark for Large Language Models</div> <div class="author"> <a href="https://jwkirchenbauer.notion.site/" rel="external nofollow noopener" target="_blank">John Kirchenbauer</a>, <em>Jonas Geiping</em>, <a href="https://yuxinwenrick.github.io/" rel="external nofollow noopener" target="_blank">Yuxin Wen</a>, Jonathan Katz, Ian Miers, and <a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="geiping_stochastic_2021" class="col-sm-8"> <div class="title">Stochastic Training Is Not Necessary for Generalization</div> <div class="author"> <em>Jonas Geiping</em>, <a href="https://goldblum.github.io/" rel="external nofollow noopener" target="_blank">Micah Goldblum</a>, Phil Pope, <a href="https://sites.google.com/site/michaelmoellermath/" rel="external nofollow noopener" target="_blank">Michael Moeller</a>, and <a href="https://www.cs.umd.edu/~tomg/" rel="external nofollow noopener" target="_blank">Tom Goldstein</a> </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, Sep 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that...</p> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%6F%6E%61%73.%67%65%69%70%69%6E%67@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=206vNCEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/8284185" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/jonasgeiping" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/jonasgeiping" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> If you have research questions, feel free to reach out anytime via email or twitter! Also, I do actually read every single email. If you did not get an answer, I may be busy, or I did not think the email was addressed to me in particular (In either case, clarifying the email and resending it after a while is ok!). If you are a student appyling for a position, please also do not forget to read the openings page. </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Jonas Geiping. <a href="https://jonasgeiping.github.io//imprint/">Impressum</a>.Last updated: August 14, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>