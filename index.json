[{"authors":["admin"],"categories":null,"content":"Hello, I\u0026rsquo;m Jonas. I work in computer science as postdoctoral researcher at the University of Maryland, College Park. My background is in Mathematics, more specifically in mathematical optimization and I am interested in research that intersects current deep learning and mathematical optimization in general, but also in the implications of optimization in machine learning for the design of secure and private ML systems.\nAs such I\u0026rsquo;ve done work ranging from understanding the impact of optimization on fundamental phenomena behind generalization in deep learning, to practical optimization strategies that show vulnerabilities in user privacy for federated learning.\nYou can find code for our research here: github.com/JonasGeiping\nFor recent publications, check here: scholar.google.com/citations?user=206vNCEAAAAJ\nIf you have research questions, feel free to reach out anytime via email or twitter!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jonasgeiping.github.io/author/jonas-geiping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonas-geiping/","section":"authors","summary":"Hello, I\u0026rsquo;m Jonas. I work in computer science as postdoctoral researcher at the University of Maryland, College Park. My background is in Mathematics, more specifically in mathematical optimization and I am interested in research that intersects current deep learning and mathematical optimization in general, but also in the implications of optimization in machine learning for the design of secure and private ML systems.","tags":null,"title":"Jonas Geiping","type":"authors"},{"authors":[],"categories":null,"content":"","date":1670198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670198400,"objectID":"15d78d8dcac13118fe2d4be1534ff91f","permalink":"https://jonasgeiping.github.io/talk/neurips2022/","publishdate":"2022-12-05T00:00:00Z","relpermalink":"/talk/neurips2022/","section":"talk","summary":"Presented ”Autoregressive Perturbations for Data Poisoning” and discussed our ongoing work in security at the 2022 NeurIPS ML Safety Workshop.","tags":[],"title":"Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)","type":"talk"},{"authors":["Kanchana Vaishnavi Gandikota","Jonas Geiping","Zorah Lähner","Adam Czaplinski","Michael Moeller"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"dec545878d3ac3d45c2cad7680064ef7","permalink":"https://jonasgeiping.github.io/publication/gandikota-simple-2022/","publishdate":"2023-01-18T23:35:57.447675Z","relpermalink":"/publication/gandikota-simple-2022/","section":"publication","summary":"Many applications require robustness, or ideally invariance, of neural networks to certain transformations of input data. Most commonly, this requirement is addressed by training data augmentation, using adversarial training, or defining network architectures that include the desired invariance by design. In this work, we propose a method to make network architectures provably invariant with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. Further, we empirically analyze the properties of different approaches which incorporate invariance via training or architecture, and demonstrate the advantages of our method in terms of robustness and computational efficiency. In particular, we investigate the robustness with respect to rotations of images (which can hold up to discretization artifacts) as well as the provable orientation and scaling invariance of 3D point cloud classification.","tags":["Computer Science - Computer Vision and Pattern Recognition"],"title":"A Simple Strategy to Provable Invariance via Orbit Mapping","type":"publication"},{"authors":["Pedro Sandoval-Segura","Vasu Singla","Jonas Geiping","Micah Goldblum","Tom Goldstein","David W. Jacobs"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"b9b342e416a411768dd7e22e5d5b0123","permalink":"https://jonasgeiping.github.io/publication/sandoval-segura-autoregressive-2022/","publishdate":"2023-01-18T23:35:57.45176Z","relpermalink":"/publication/sandoval-segura-autoregressive-2022/","section":"publication","summary":"The prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. Data poisoning attacks have been proposed as a bulwark against scraping, as they make data ``unlearnable'' by adding small, imperceptible perturbations. Unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. In this work, we introduce autoregressive (AR) poisoning, a method that can generate poisoned data without access to the broader dataset. The proposed AR perturbations are generic, can be applied across different datasets, and can poison different architectures. Compared to existing unlearnable methods, our AR poisons are more resistant against common defenses such as adversarial training and strong data augmentations. Our analysis further provides insight into what makes an effective data poison.","tags":null,"title":"Autoregressive Perturbations for Data Poisoning","type":"publication"},{"authors":["Jonas Geiping","Tom Goldstein"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"14cbae61cd09ad9f58c3dd54c772c3a5","permalink":"https://jonasgeiping.github.io/publication/geiping-cramming-2022/","publishdate":"2023-01-18T23:35:57.448435Z","relpermalink":"/publication/geiping-cramming-2022/","section":"publication","summary":"Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.","tags":["Computer Science - Computation and Language","Computer Science - Machine Learning"],"title":"Cramming: Training a Language Model on a Single GPU in One Day","type":"publication"},{"authors":["Gowthami Somepalli","Vasu Singla","Micah Goldblum","Jonas Geiping","Tom Goldstein"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"af13d06297bba380df0d486d40afc219","permalink":"https://jonasgeiping.github.io/publication/somepalli-diffusion-2022/","publishdate":"2023-01-18T23:35:57.452127Z","relpermalink":"/publication/somepalli-diffusion-2022/","section":"publication","summary":"Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they stealing content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Computers and Society","Computer Science - Machine Learning"],"title":"Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models","type":"publication"},{"authors":["Hong-Min Chu","Jonas Geiping","Liam Fowl","Micah Goldblum","Tom Goldstein"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"970b07a6adcc25480841a8cd97b220c0","permalink":"https://jonasgeiping.github.io/publication/chu-panning-2022/","publishdate":"2023-01-18T23:35:57.446502Z","relpermalink":"/publication/chu-panning-2022/","section":"publication","summary":"As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase ``my credit card number is ...\". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.","tags":null,"title":"Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation","type":"publication"},{"authors":["Neel Jain","John Kirchenbauer","Jonas Geiping","Tom Goldstein"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"cbaaffc15201ec9571ca0542350932b8","permalink":"https://jonasgeiping.github.io/publication/jain-how-2022/","publishdate":"2023-01-18T23:35:57.451408Z","relpermalink":"/publication/jain-how-2022/","section":"publication","summary":"There are a wide range of different tokenizers and vocabularies that have been used to train language models, and training a language model on just one of these can be prohibitively expensive. The ability to swap the vocabulary of a model after it has been trained enables models to be adapted to different tokenizers, and even different languages, without the computational or data cost of from-scratch training. In this paper, we ask when such swaps are possible, and how to perform them effectively? The major challenge of performing a vocab swap is re-learning the parameters of the embedding layer for the vocabulary. We observe that it is possible to re-learn the embedding for a vocabulary using a naive initialization, and we investigate strong initialization strategies that enable learning of new embeddings for swapped vocabularies, even when those vocabularies come from a different source language than the original language model.","tags":["⛔ No DOI found"],"title":"How to Do a Vocab Swap? A Study of Embedding Replacement for Pre-trained Transformers","type":"publication"},{"authors":[],"categories":null,"content":"","date":1666828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666828800,"objectID":"655085bd9bac05391928b9df7f60ac15","permalink":"https://jonasgeiping.github.io/talk/qualcomm2022/","publishdate":"2022-10-27T00:00:00Z","relpermalink":"/talk/qualcomm2022/","section":"talk","summary":"Happy to be an invited speaker at the Qualcomm AI seminar. I gave a talk titled `Privacy and Security Analysis in Federated Learning` about our recent work in federated learning.","tags":[],"title":"DistributedML Seminar at Qualcomm AI","type":"talk"},{"authors":["Yuxin Wen","Arpit Bansal","Hamid Kazemi","Eitan Borgnia","Micah Goldblum","Jonas Geiping","Tom Goldstein"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"f8f5a011ad0c2ec5ceceeceb5ddd9987","permalink":"https://jonasgeiping.github.io/publication/wen-canary-2022/","publishdate":"2023-01-18T23:35:57.452313Z","relpermalink":"/publication/wen-canary-2022/","section":"publication","summary":"As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings. Code is available at https://github.com/YuxinWenRick/canary-in-a-coalmine.","tags":["Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries","type":"publication"},{"authors":["Jonas Geiping","Micah Goldblum","Gowthami Somepalli","Ravid Shwartz-Ziv","Tom Goldstein","Andrew Gordon Wilson"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"f79ce6ce79359e186f50b074d440e397","permalink":"https://jonasgeiping.github.io/publication/geiping-how-2022/","publishdate":"2023-01-18T23:35:57.449127Z","relpermalink":"/publication/geiping-how-2022/","section":"publication","summary":"Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Machine Learning"],"title":"How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization","type":"publication"},{"authors":["Renkun Ni","Ping-yeh Chiang","Jonas Geiping","Micah Goldblum","Andrew Gordon Wilson","Tom Goldstein"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"63939c32cbb5b8a9340586f2de62e953","permalink":"https://jonasgeiping.github.io/publication/ni-k-sam-2022/","publishdate":"2023-01-18T23:35:57.451586Z","relpermalink":"/publication/ni-k-sam-2022/","section":"publication","summary":"Sharpness-Aware Minimization (SAM) has recently emerged as a robust technique for improving the accuracy of deep neural networks. However, SAM incurs a high computational cost in practice, requiring up to twice as much computation as vanilla SGD. The computational challenge posed by SAM arises because each iteration requires both ascent and descent steps and thus double the gradient computations. To address this challenge, we propose to compute gradients in both stages of SAM on only the top-k samples with highest loss. K-SAM is simple and extremely easy-to-implement while providing significant generalization boosts over vanilla SGD at little to no additional cost.","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Machine Learning"],"title":"K-SAM: Sharpness-Aware Minimization at the Speed of SGD","type":"publication"},{"authors":["Yuxin Wen","Jonas Geiping","Liam Fowl","Hossein Souri","Rama Chellappa","Micah Goldblum","Tom Goldstein"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"bfa51dab171b31ff5e005a729197155c","permalink":"https://jonasgeiping.github.io/publication/wen-thinking-2022/","publishdate":"2023-01-18T23:35:57.452678Z","relpermalink":"/publication/wen-thinking-2022/","section":"publication","summary":"Federated learning is particularly susceptible to model poisoning and backdoor attacks because individual users have direct control over the training data and model updates. At the same time, the attack power of an individual user is limited because their updates are quickly drowned out by those of many other users. Existing attacks do not account for future behaviors of other users, and thus require many sequential updates and their effects are quickly erased. We propose an attack that anticipates and accounts for the entire federated learning pipeline, including behaviors of other clients, and ensures that backdoors are effective quickly and persist even after multiple rounds of community updates. We show that this new attack is effective in realistic scenarios where the attacker only contributes to a small fraction of randomly sampled rounds and demonstrate this attack on image classification, next-word prediction, and sentiment analysis.","tags":["Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor Attacks in Federated Learning","type":"publication"},{"authors":["Arpit Bansal","Eitan Borgnia","Hong-Min Chu","Jie S. Li","Hamid Kazemi","Furong Huang","Micah Goldblum","Jonas Geiping","Tom Goldstein"],"categories":null,"content":"","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"68d3fed68933fcea9beb9a71a686d97e","permalink":"https://jonasgeiping.github.io/publication/bansal-cold-2022/","publishdate":"2023-01-18T23:35:57.445501Z","relpermalink":"/publication/bansal-cold-2022/","section":"publication","summary":"Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Machine Learning"],"title":"Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise","type":"publication"},{"authors":["Yuxin Wen","Jonas Geiping","Liam Fowl","Micah Goldblum","Tom Goldstein"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"26f6bffc64075087c6fd1390dfc412c8","permalink":"https://jonasgeiping.github.io/publication/wen-fishing-2022/","publishdate":"2023-01-18T23:35:57.452493Z","relpermalink":"/publication/wen-fishing-2022/","section":"publication","summary":"Federated learning (FL) has rapidly risen in popularity due to its promise of privacy and efficiency. Previous works have exposed privacy vulnerabilities in the FL pipeline by recovering user data from gradient updates. However, existing attacks fail to address realistic settings because they either 1) require toy settings with very small batch sizes, or 2) require unrealistic and conspicuous architecture modifications. We introduce a new strategy that dramatically elevates existing attacks to operate on batches of arbitrarily large size, and without architectural modifications. Our model-agnostic strategy only requires modifications to the model parameters sent to the user, which is a realistic threat model in many scenarios. We demonstrate the strategy in challenging large-scale settings, obtaining high-fidelity data extraction in both cross-device and cross-silo federated learning. Code is available at https://github.com/JonasGeiping/breaching.","tags":null,"title":"Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification","type":"publication"},{"authors":["Pedro Sandoval-Segura","Vasu Singla","Liam Fowl","Jonas Geiping","Micah Goldblum","David Jacobs","Tom Goldstein"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"e42e5b465a22455ee7676efd64599a0e","permalink":"https://jonasgeiping.github.io/publication/sandoval-segura-poisons-2022/","publishdate":"2023-01-18T23:35:57.451949Z","relpermalink":"/publication/sandoval-segura-poisons-2022/","section":"publication","summary":"Imperceptible poisoning attacks on entire datasets have recently been touted as methods for protecting data privacy. However, among a number of defenses preventing the practical use of these techniques, early-stopping stands out as a simple, yet effective defense. To gauge poisons' vulnerability to early-stopping, we benchmark error-minimizing, error-maximizing, and synthetic poisons in terms of peak test accuracy over 100 epochs and make a number of surprising observations. First, we find that poisons that reach a low training loss faster have lower peak test accuracy. Second, we find that a current state-of-the-art error-maximizing poison is 7× less effective when poison training is stopped at epoch 8. Third, we find that stronger, more transferable adversarial attacks do not make stronger poisons. We advocate for evaluating poisons in terms of peak test accuracy.","tags":["Computer vision","Correlation","Data privacy","Perturbation methods","Privacy","Toxicology","Training"],"title":"Poisons That Are Learned Faster Are More Effective","type":"publication"},{"authors":[],"categories":null,"content":"","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653436800,"objectID":"bd60ef19112413ca363ed3415fad9556","permalink":"https://jonasgeiping.github.io/talk/flowtalk/","publishdate":"2022-05-25T00:00:00Z","relpermalink":"/talk/flowtalk/","section":"talk","summary":"Presented recent work for privacy attacks in malicious server threat models at the Federated Learning One World (FLOW) Seminar.","tags":[],"title":"Federated Learning One World (FLOW) Seminar","type":"talk"},{"authors":[],"categories":null,"content":"","date":1650844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650844800,"objectID":"d756345d8910f042c941cf02cc49a626","permalink":"https://jonasgeiping.github.io/talk/iclr2022/","publishdate":"2022-04-25T00:00:00Z","relpermalink":"/talk/iclr2022/","section":"talk","summary":"We presented our work ”Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models” and our work ”Stochastic Training is Not Necessary for Generalization” at ICLR 2022. I was named 'Highlighted Reviewer of ICLR 2022' for which I am very grateful.","tags":[],"title":"Tenth International Conference on Learning Representations (ICLR 2022)","type":"talk"},{"authors":[],"categories":null,"content":"","date":1645660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645660800,"objectID":"0e5bc6fc7c331d00396d7e0155900476","permalink":"https://jonasgeiping.github.io/talk/riken2022/","publishdate":"2022-02-24T00:00:00Z","relpermalink":"/talk/riken2022/","section":"talk","summary":"Presented recent work for privacy attacks in Federated Learning at RIKEN AIP's 5th TrustML Young Scientist Seminar. Discussed ”Inverting Gradients - How easy is it to break privacy in FL” and recent works on malicious server models (Robbing the Fed, Decepticons, Fishing for User Data).","tags":[],"title":"The 5th TrustML Young Scientist Seminar","type":"talk"},{"authors":["Liam Fowl","Jonas Geiping","Steven Reich","Yuxin Wen","Wojtek Czaja","Micah Goldblum","Tom Goldstein"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"85bcdedf0b33deaf47181edc805e1e81","permalink":"https://jonasgeiping.github.io/publication/fowl-decepticons-2022/","publishdate":"2023-01-18T23:35:57.447014Z","relpermalink":"/publication/fowl-decepticons-2022/","section":"publication","summary":"A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.","tags":["Computer Science - Computation and Language","Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models","type":"publication"},{"authors":[],"categories":null,"content":"","date":1638835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638835200,"objectID":"440f808572e1dfaa4fe56596a17e133d","permalink":"https://jonasgeiping.github.io/talk/neurips2021/","publishdate":"2021-12-07T00:00:00Z","relpermalink":"/talk/neurips2021/","section":"talk","summary":"Our recent work ”Adversarial Examples Make Strong Poisons” was presented here. Also discussed ”DARTS - A study on hyperparameter stability” at the Deep Learning and Inverse Problems workshop.","tags":[],"title":"Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)","type":"talk"},{"authors":[],"categories":null,"content":"","date":1636329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636329600,"objectID":"5aa40732dc460b44f75035ec35ddce82","permalink":"https://jonasgeiping.github.io/talk/flworkshop2021/","publishdate":"2021-11-08T00:00:00Z","relpermalink":"/talk/flworkshop2021/","section":"talk","summary":"Participated in Google’s 2021 Workshop on Federated Learning and Analytics and presented a poster for our recent work ”Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models”.","tags":[],"title":"2021 Workshop on Federated Learning and Analytics","type":"talk"},{"authors":["Liam Fowl","Jonas Geiping","Wojciech Czaja","Micah Goldblum","Tom Goldstein"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"f453791d23295e65b4cda23cb05e1f20","permalink":"https://jonasgeiping.github.io/publication/fowl-robbing-2021/","publishdate":"2023-01-18T23:35:57.447485Z","relpermalink":"/publication/fowl-robbing-2021/","section":"publication","summary":"Federated learning has quickly gained popularity with its promises of increased user privacy and efficiency.  Previous works have shown that federated gradient updates contain information that can...","tags":["Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models","type":"publication"},{"authors":["Jonas Geiping","Micah Goldblum","Phil Pope","Michael Moeller","Tom Goldstein"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"982ab9d5d1eb7054e4a025ae41fcf52d","permalink":"https://jonasgeiping.github.io/publication/geiping-stochastic-2021/","publishdate":"2023-01-18T23:35:57.45014Z","relpermalink":"/publication/geiping-stochastic-2021/","section":"publication","summary":"It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks.  In this work, we demonstrate that...","tags":["Computer Science - Machine Learning","Mathematics - Optimization and Control"],"title":"Stochastic Training Is Not Necessary for Generalization","type":"publication"},{"authors":["Jonas Geiping","Jovita Lukasik","Margret Keuper","Michael Moeller"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"6c1c9da6f9b5555e0e4e3b44b8e9b510","permalink":"https://jonasgeiping.github.io/publication/geiping-darts-2021/","publishdate":"2023-01-18T23:35:57.44861Z","relpermalink":"/publication/geiping-darts-2021/","section":"publication","summary":"Differentiable architecture search (DARTS) is a widely researched tool for neural architecture search, due to its promising results for image classification. The main benefit of DARTS is the effectiveness achieved through the weight-sharing one-shot paradigm, which allows efficient architecture search. In this work, we investigate DARTS in a systematic case study of inverse problems, which allows us to analyze these potential benefits in a controlled manner. Although we demonstrate that the success of DARTS can be extended from image classification to reconstruction, our experiments yield three fundamental difficulties in the evaluation of DARTS-based methods: First, the results show a large variance in all test cases. Second, the final performance is highly dependent on the hyperparameters of the optimizer. And third, the performance of the weight-sharing architecture used during training does not reflect the final performance of the found architecture well. Thus, we conclude the necessity to 1) report the results of any DARTS-based methods from several runs along with its underlying performance statistics, 2) show the correlation of the training and final architecture performance, and 3) carefully consider if the computational efficiency of DARTS outweighs the costs of hyperparameter optimization and multiple runs.","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Machine Learning"],"title":"DARTS for Inverse Problems: A Study on Hyperparameter Sensitivity","type":"publication"},{"authors":[],"categories":null,"content":"","date":1624492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624492800,"objectID":"ecdfe53f132ed6386c04fb0f185a9201","permalink":"https://jonasgeiping.github.io/talk/google2021/","publishdate":"2021-06-24T00:00:00Z","relpermalink":"/talk/google2021/","section":"talk","summary":"Presented and discussed recent work on privacy attacks in Federated Learning.","tags":[],"title":"Google's Privacy Testing Research Meeting","type":"talk"},{"authors":["Eitan Borgnia","Valeriia Cherepanova","Liam Fowl","Amin Ghiasi","Jonas Geiping","Micah Goldblum","Tom Goldstein","Arjun Gupta"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"7f7e8086789fa397aada823bda6c2914","permalink":"https://jonasgeiping.github.io/publication/borgnia-strong-2021/","publishdate":"2023-01-18T23:35:57.446067Z","relpermalink":"/publication/borgnia-strong-2021/","section":"publication","summary":"Data poisoning and backdoor attacks manipulate victim models by maliciously modifying training data. In light of this growing threat, a recent survey of industry professionals revealed heightened fear in the private sector regarding data poisoning. Many previous defenses against poisoning either fail in the face of increasingly strong attacks, or they significantly degrade performance. However, we find that strong data augmentations, such as mixup and CutMix, can significantly diminish the threat of poisoning and backdoor attacks without trading off performance. We further verify the effectiveness of this simple defense against adaptive poisoning methods, and we compare to baselines including the popular differentially private SGD (DP-SGD) defense. In the context of backdoors, CutMix greatly mitigates the attack while simultaneously increasing validation accuracy by 9%.","tags":["Adversarial Attacks","Backdoor Attacks","Computer Science - Cryptography and Security","Computer Science - Machine Learning","Conferences","Data Augmentation","Data models","Data Poisoning","Differential Privacy","Industries","Machine learning","Signal processing","Toxicology","Training data"],"title":"Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff","type":"publication"},{"authors":["Kanchana Vaishnavi Gandikota","Jonas Geiping","Zorah Lähner","Adam Czaplinski","Michael Moeller"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"0f6611cbdc27649e9c56dde24624ef65","permalink":"https://jonasgeiping.github.io/publication/gandikota-training-2021/","publishdate":"2023-01-18T23:35:57.447867Z","relpermalink":"/publication/gandikota-training-2021/","section":"publication","summary":"Many applications require the robustness, or ideally the invariance, of a neural network to certain transformations of input data. Most commonly, this requirement is addressed by either augmenting the training data, using adversarial training, or defining network architectures that include the desired invariance automatically. Unfortunately, the latter often relies on the ability to enlist all possible transformations, which make such approaches largely infeasible for infinite sets of transformations, such as arbitrary rotations or scaling. In this work, we propose a method for provably invariant network architectures with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. We analyze properties of such approaches, extend them to equivariant networks, and demonstrate their advantages in terms of robustness as well as computational efficiency in several numerical examples. In particular, we investigate the robustness with respect to rotations of images (which can possibly hold up to discretization artifacts only) as well as the provable rotational and scaling invariance of 3D point cloud classification.","tags":["Computer Science - Computer Vision and Pattern Recognition"],"title":"Training or Architecture? How to Incorporate Invariance in Neural Networks","type":"publication"},{"authors":[],"categories":null,"content":"","date":1619308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619308800,"objectID":"df5db4ba7cacb11ff9e8287bffd3d155","permalink":"https://jonasgeiping.github.io/talk/iclr2021/","publishdate":"2021-04-25T00:00:00Z","relpermalink":"/talk/iclr2021/","section":"talk","summary":"Discussed our work ”Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching” at the main conference and discussed our prelimary ideas for defenses at the Security workshop.","tags":[],"title":"Ninth International Conference on Learning Representations (ICLR 2021)","type":"talk"},{"authors":["Jonas Geiping","Liam H. Fowl","W. Ronny Huang","Wojciech Czaja","Gavin Taylor","Michael Moeller","Tom Goldstein"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"5c293439ab67bb1ec5704f0cd83d4402","permalink":"https://jonasgeiping.github.io/publication/geiping-witches-2021/","publishdate":"2023-01-18T23:35:57.450485Z","relpermalink":"/publication/geiping-witches-2021/","section":"publication","summary":"Data Poisoning attacks modify training data to maliciously control a model trained on such data. Previous poisoning attacks against deep neural networks have been limited in scope and success...","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Machine Learning"],"title":"Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching","type":"publication"},{"authors":["Eitan Borgnia","Jonas Geiping","Valeriia Cherepanova","Liam Fowl","Arjun Gupta","Amin Ghiasi","Furong Huang","Micah Goldblum","Tom Goldstein"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"2f25913a08b688a3d94c99b3f9f1bd4b","permalink":"https://jonasgeiping.github.io/publication/borgnia-dp-instahide-2021/","publishdate":"2023-01-18T23:35:57.445856Z","relpermalink":"/publication/borgnia-dp-instahide-2021/","section":"publication","summary":"Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this work, we show that strong data augmentations, such as mixup and random additive noise, nullify poison attacks while enduring only a small accuracy trade-off. To explain these finding, we propose a training method, DP-InstaHide, which combines the mixup regularizer with additive noise. A rigorous analysis of DP-InstaHide shows that mixup does indeed have privacy advantages, and that training with k-way mixup provably yields at least k times stronger DP guarantees than a naive DP mechanism. Because mixup (as opposed to noise) is beneficial to model performance, DP-InstaHide provides a mechanism for achieving stronger empirical performance against poisoning attacks than other known DP methods.","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations","type":"publication"},{"authors":["Liam Fowl","Ping-yeh Chiang","Micah Goldblum","Jonas Geiping","Arpit Bansal","Wojtek Czaja","Tom Goldstein"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"a069e2f7c1e6d5ced82889697997b510","permalink":"https://jonasgeiping.github.io/publication/fowl-preventing-2021/","publishdate":"2023-01-18T23:35:57.44724Z","relpermalink":"/publication/fowl-preventing-2021/","section":"publication","summary":"Large organizations such as social media companies continually release data, for example user images. At the same time, these organizations leverage their massive corpora of released data to train proprietary models that give them an edge over their competitors. These two behaviors can be in conflict as an organization wants to prevent competitors from using their own data to replicate the performance of their proprietary models. We solve this problem by developing a data poisoning method by which publicly released data can be minimally modified to prevent others from train-ing models on it. Moreover, our method can be used in an online fashion so that companies can protect their data in real time as they release it.We demonstrate the success of our approach onImageNet classification and on facial recognition.","tags":["Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release","type":"publication"},{"authors":["Jonas Geiping","Liam Fowl","Gowthami Somepalli","Micah Goldblum","Michael Moeller","Tom Goldstein"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"a90f65b0d7d12eb34ced86183628b1e7","permalink":"https://jonasgeiping.github.io/publication/geiping-what-2021/","publishdate":"2023-01-18T23:35:57.450313Z","relpermalink":"/publication/geiping-what-2021/","section":"publication","summary":"Data poisoning is a threat model in which a malicious actor tampers with training data to manipulate outcomes at inference time. A variety of defenses against this threat model have been proposed, but each suffers from at least one of the following flaws: they are easily overcome by adaptive attacks, they severely reduce testing performance, or they cannot generalize to diverse data poisoning threat models. Adversarial training, and its variants, is currently considered the only empirically strong defense against (inference-time) adversarial attacks. In this work, we extend the adversarial training framework to instead defend against (training-time) poisoning and backdoor attacks. Our method desensitizes networks to the effects of poisoning by creating poisons during training and injecting them into training batches. We show that this defense withstands adaptive attacks, generalizes to diverse threat models, and incurs a better performance trade-off than previous defenses.","tags":["Computer Science - Computer Vision and Pattern Recognition","Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"What Doesn't Kill You Makes You Robust(Er): Adversarial Training against Poisons and Backdoors","type":"publication"},{"authors":["Liam Fowl","Micah Goldblum","Ping-yeh Chiang","Jonas Geiping","Wojciech Czaja","Tom Goldstein"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"713cdff0417dcd6cd4be9cda8bf3f708","permalink":"https://jonasgeiping.github.io/publication/fowl-adversarial-2021/","publishdate":"2023-01-18T23:35:57.446788Z","relpermalink":"/publication/fowl-adversarial-2021/","section":"publication","summary":"The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data.  In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. In fact, adversarial examples with labels re-assigned by the crafting network remain effective for training, suggesting that adversarial examples contain useful semantic content, just with the \"wrong\" labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.","tags":["⛔ No DOI found","Computer Science - Cryptography and Security","Computer Science - Machine Learning"],"title":"Adversarial Examples Make Strong Poisons","type":"publication"},{"authors":["Jonas Geiping"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"fa1c1e14797f2ce6c365b6586f64e4b2","permalink":"https://jonasgeiping.github.io/publication/geiping-modern-2021/","publishdate":"2023-01-18T23:35:57.44968Z","relpermalink":"/publication/geiping-modern-2021/","section":"publication","summary":"This thesis presents research into multiple optimization topics in computer vision with a conceptual focus on composite optimization problems such as bilevel optimization. The optimal graph-based discretization of variational problems in minimal partitions, the theoretical analysis of nonconvex composite optimization by nonconvex majorizers, the bilevel problem of learning energy models by nonconvex majorizers, and the machine learning security applications of bilevel optimization in privacy analysis of federated learning and dataset poisoning of general image classification are featured in this cumulative work.","tags":null,"title":"Modern Optimization Techniques in Computer Vision","type":"publication"},{"authors":[],"categories":null,"content":"","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607212800,"objectID":"1468c4819a67d577f48c4f3959e475cd","permalink":"https://jonasgeiping.github.io/talk/neurips2020/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/talk/neurips2020/","section":"talk","summary":"Presented our recent work ”Inverting Gradients - How easy is it to break privacy in federated learning?”.","tags":[],"title":"Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020)","type":"talk"},{"authors":["Jonas Geiping","Hartmut Bauermeister","Hannah Dröge","Michael Moeller"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"e7c4c8a58d9de6e178c71bb83e8695b9","permalink":"https://jonasgeiping.github.io/publication/geiping-inverting-2020/","publishdate":"2023-01-18T23:35:57.449509Z","relpermalink":"/publication/geiping-inverting-2020/","section":"publication","summary":"","tags":null,"title":"Inverting Gradients - How Easy Is It to Break Privacy in Federated Learning?","type":"publication"},{"authors":["W. Ronny Huang","Jonas Geiping","Liam Fowl","Gavin Taylor","Tom Goldstein"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"56a7e75230c57dedf0c4f48547f8b728","permalink":"https://jonasgeiping.github.io/publication/huang-metapoison-2020/","publishdate":"2023-01-18T23:35:57.451127Z","relpermalink":"/publication/huang-metapoison-2020/","section":"publication","summary":"","tags":["Computer Science - Artificial Intelligence","Computer Science - Computer Vision and Pattern Recognition","Computer Science - Cryptography and Security","Computer Science - Machine Learning","Statistics - Machine Learning"],"title":"MetaPoison: Practical General-purpose Clean-label Data Poisoning","type":"publication"},{"authors":[],"categories":null,"content":"","date":1599436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599436800,"objectID":"c508d3b90d72b37301666273fda312fc","permalink":"https://jonasgeiping.github.io/talk/bmvc2020/","publishdate":"2020-09-07T00:00:00Z","relpermalink":"/talk/bmvc2020/","section":"talk","summary":"Presented ”Fast Convex Relaxations via Graph Discretizations” as oral presentation at BMVC 2020.","tags":[],"title":"31st British Machine Vision Conference (BMVC 2020)","type":"talk"},{"authors":["Jonas Geiping","Fjedor Gaede","Hartmut Bauermeister","Michael Moeller"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"3db9b5e543b593808333061dcf4df050","permalink":"https://jonasgeiping.github.io/publication/geiping-fast-2020/","publishdate":"2023-01-18T23:35:57.448952Z","relpermalink":"/publication/geiping-fast-2020/","section":"publication","summary":"Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10.","tags":["Computer Science - Computer Vision and Pattern Recognition","Mathematics - Optimization and Control"],"title":"Fast Convex Relaxations Using Graph Discretizations","type":"publication"},{"authors":[],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"1db25fb35c3cdebf06f20e89ac8c71c3","permalink":"https://jonasgeiping.github.io/talk/siam2020/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/talk/siam2020/","section":"talk","summary":"Talked about at the SIAM MDS2020 minisymposium ”Learning parameterized energy minimization models” about ”Parametric Majorization for Data-Driven Energy Minimization Methods”.","tags":[],"title":"SIAM Conference on Mathematics of Data Science 2020","type":"talk"},{"authors":["Ping-Yeh Chiang","Jonas Geiping","Micah Goldblum","Tom Goldstein","Renkun Ni","Steven Reich","Ali Shafahi"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"25a1e8832db3e4f1644759c8e16e443e","permalink":"https://jonasgeiping.github.io/publication/chiang-witchcraft-2020/","publishdate":"2023-01-18T23:35:57.446261Z","relpermalink":"/publication/chiang-witchcraft-2020/","section":"publication","summary":"State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.","tags":["Adversarial","adversarial attacks","Attack","CIFAR","classical PGD attack","CNN","Computer Science - Computer Vision and Pattern Recognition","Computer Science - Cryptography and Security","Computer Science - Machine Learning","Electrical Engineering and Systems Science - Signal Processing","gradient methods","iterative FGSM-based methods","iterative methods","learning (artificial intelligence)","neural nets","neural networks","PGD","PGD attacks","projected gradient descent","random step size","Statistics - Machine Learning","stochastic processes","wide iterative stochastic crafting","witchcraft"],"title":"Witchcraft: Efficient PGD Attacks with Random Step Size","type":"publication"},{"authors":[],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"d3299ad91945ca4df0b73eb439b85fa9","permalink":"https://jonasgeiping.github.io/talk/iclr2020/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/talk/iclr2020/","section":"talk","summary":"Discussed our oral presentation ”Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory”.","tags":[],"title":"Eighth International Conference on Learning Representations (ICLR 2020)","type":"talk"},{"authors":["Micah Goldblum","Jonas Geiping","Avi Schwarzschild","Michael Moeller","Tom Goldstein"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"84223e74de49a26ee95ab5a09094b71a","permalink":"https://jonasgeiping.github.io/publication/goldblum-truth-2020/","publishdate":"2023-01-18T23:35:57.450661Z","relpermalink":"/publication/goldblum-truth-2020/","section":"publication","summary":"We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.","tags":["Computer Science - Machine Learning","Mathematics - Optimization and Control","Statistics - Machine Learning"],"title":"Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory","type":"publication"},{"authors":["Andreas Görlitz","Jonas Geiping","Andreas Kolb"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"9669837da2d3bc85123d11d445873cae","permalink":"https://jonasgeiping.github.io/publication/gorlitz-piecewise-2019-1/","publishdate":"2023-01-18T23:35:57.45085Z","relpermalink":"/publication/gorlitz-piecewise-2019-1/","section":"publication","summary":"In this paper, we introduce a novel variational approach to estimate the scene flow from RGB-D images. We regularize the ill-conditioned problem of scene flow estimation in a unified framework by enforcing piecewise rigid motion through decomposition into rotational and translational motion parts. Our model crucially regularizes these components by an L0 ``norm'', thereby facilitating implicit motion segmentation in a joint energy minimization problem. Yet, we also show that this energy can be efficiently minimized by a proximal primal-dual algorithm. By implementing this approximate L0 rigid motion regularization, our scene flow estimation approach implicitly segments the observed scene of into regions of nearly constant rigid motion. We evaluate our joint scene flow and segmentation estimation approach on a variety of test scenarios, with and without ground truth data, and demonstrate that we outperform current scene flow techniques.","tags":["constant rigid motion","ill-conditioned problem","image colour analysis","image segmentation","image sequences","implicit motion segmentation estimation approach","joint energy minimization problem","L0 rigid motion regularization","minimisation","motion estimation","observed scene","piecewise rigid motion","piecewise rigid scene flow techniques","proximal primal-dual algorithm","RGB-D images","rotational motion parts","scene flow estimation approach","translational motion parts","variational approach"],"title":"Piecewise Rigid Scene Flow with Implicit Motion Segmentation","type":"publication"},{"authors":[],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"4ea9873a827b30dfcae8e35f9b902149","permalink":"https://jonasgeiping.github.io/talk/iccv2019/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/talk/iccv2019/","section":"talk","summary":"Presented ”Parametric Majorization for Data-Driven Energy Minimization Methods” at ICCV 2019.","tags":[],"title":"ICCV 2019","type":"talk"},{"authors":["Jonas Geiping","Michael Moeller"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e15c48f1f83436b34a374a0db20265ee","permalink":"https://jonasgeiping.github.io/publication/geiping-parametric-2019-1/","publishdate":"2023-01-18T23:35:57.449987Z","relpermalink":"/publication/geiping-parametric-2019-1/","section":"publication","summary":"Energy minimization methods are a classical tool in a multitude of computer vision applications. While they are interpretable and well-studied, their regularity assumptions are difficult to design by hand. Deep learning techniques on the other hand are purely data-driven, often provide excellent results, but are very difficult to constrain to predefined physical or safety-critical models. A possible combination between the two approaches is to design a parametric en- ergy and train the free parameters in such a way that minimizers of the energy correspond to desired solution on a set of training examples. Unfortunately, such formulations typically lead to bi-level optimization problems, on which common optimization algorithms are difficult to scale to modern requirements in data processing and efficiency. In this work, we present a new strategy to optimize these bi-level problems. We investigate surrogate single-level problems that majorize the target problems and can be implemented with existing tools, leading to efficient algorithms without collapse of the energy function. This framework of strategies enables new avenues to the training of parameterized energy minimization models from large data.","tags":["90C06; 68U10; 68T45; 65K10","Computer Science - Computer Vision and Pattern Recognition","Computer Science - Machine Learning","G.1.6","G.4","I.4","Mathematics - Optimization and Control"],"title":"Parametric Majorization for Data-Driven Energy Minimization Methods","type":"publication"},{"authors":[],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"aae836ed73c1a55e3e271f9a144f6c43","permalink":"https://jonasgeiping.github.io/talk/ifip/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/talk/ifip/","section":"talk","summary":"Visited the IFIP TC 7 Conference on System Modelling and Optimization.","tags":[],"title":"IFIP TC 7 Conference on System Modelling and Optimization","type":"talk"},{"authors":[],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"dc3258394dd125f1823e9de85c3cb206","permalink":"https://jonasgeiping.github.io/talk/icml2018/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/talk/icml2018/","section":"talk","summary":"Visited the International Conference on Machine Learning.","tags":[],"title":"International Conference on Machine Learning, 2018","type":"talk"},{"authors":null,"categories":null,"content":"My webpage does not collect your data :\u0026gt;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://jonasgeiping.github.io/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"My webpage does not collect your data :\u0026gt;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":[],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"f63e17d0b0f33328c6d58e4cebbf7508","permalink":"https://jonasgeiping.github.io/talk/siam2018/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/talk/siam2018/","section":"talk","summary":"Presented the recent SIAM publication ”Composite Optimization by Nonconvex Majorization-Minimization”","tags":[],"title":"SIAM Conference on Imaging Sciences","type":"talk"},{"authors":[],"categories":null,"content":"","date":1520985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520985600,"objectID":"3c0a6aa3ad3be872160a5c37ef4d22d9","permalink":"https://jonasgeiping.github.io/talk/siegen2018/","publishdate":"2018-03-14T00:00:00Z","relpermalink":"/talk/siegen2018/","section":"talk","summary":"Presented a poster regarding composite nonconvex optimization.","tags":[],"title":"Workshop: Imaging and Vision from Theory to Applications","type":"talk"},{"authors":[],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"c9616b536e9c9a38453944bce5c92cad","permalink":"https://jonasgeiping.github.io/talk/gamm2018/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/talk/gamm2018/","section":"talk","summary":"Held a presentation titled ”Composite Optimization by Nonconvex Majorization-Minimization” at the GAMM (German Society for Applied Math and Mechanics) Annual Meeting 2018","tags":[],"title":"GAMM Annual Meeting 2018","type":"talk"},{"authors":[],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"0947794c1820433821988d2e733d9e3f","permalink":"https://jonasgeiping.github.io/talk/winteropt/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/talk/winteropt/","section":"talk","summary":"Presented a poster with preliminary work on ”Composite Optimization by Nonconvex Majorization- Minimization” and visited the winter school ”Modern Methods in Nonsmooth Optimization”.","tags":[],"title":"Winter School on Modern Methods in Nonsmooth Optimization","type":"talk"},{"authors":["Jonas Geiping","Michael Moeller"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"479a10e4de17ceeead3cd2847cfde441","permalink":"https://jonasgeiping.github.io/publication/geiping-composite-2018/","publishdate":"2023-01-18T23:35:57.448258Z","relpermalink":"/publication/geiping-composite-2018/","section":"publication","summary":"The minimization of a nonconvex composite function can model a variety of imaging tasks. A popular class of algorithms for solving such problems are majorization-minimization techniques which iteratively approximate the composite nonconvex function by a majorizing function that is easy to minimize. Most techniques, e.g., gradient descent, utilize convex majorizers in order to guarantee that the majorizer is easy to minimize. In our work we consider a natural class of nonconvex majorizers for these functions, and show that these majorizers are still sufficient for a globally convergent optimization scheme. Numerical results illustrate that by applying this scheme, one can often obtain superior local optima compared to previous majorization-minimization methods, when the nonconvex majorizers are solved to global optimality. Finally, we illustrate the behavior of our algorithm for depth superresolution from raw time-of-flight data.","tags":["90C26; 90C06; 68U10; 32B20; 65K10; 47J06","Computer Science - Computer Vision and Pattern Recognition","Mathematics - Numerical Analysis","Mathematics - Optimization and Control"],"title":"Composite Optimization by Nonconvex Majorization-Minimization","type":"publication"},{"authors":["Jonas Geiping","Hendrik Dirks","Daniel Cremers","Michael Moeller"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"75fe9ac2a800ad756fc66fd2608ac2e9","permalink":"https://jonasgeiping.github.io/publication/geiping-multiframe-2018/","publishdate":"2023-01-18T23:35:57.449838Z","relpermalink":"/publication/geiping-multiframe-2018/","section":"publication","summary":"The idea of video super resolution is to use different view points of a single scene to enhance the overall resolution and quality. Classical energy minimization approaches first establish a correspondence of the current frame to all its neighbors in some radius and then use this temporal information for enhancement. In this paper, we propose the first variational super resolution approach that computes several super resolved frames in one batch optimization procedure by incorporating motion information between the high-resolution image frames themselves. As a consequence, the number of motion estimation problems grows linearly in the number of frames, opposed to a quadratic growth of classical methods and temporal consistency is enforced naturally.We use infimal convolution regularization as well as an automatic parameter balancing scheme to automatically determine the reliability of the motion information and reweight the regularization locally. We demonstrate that our approach yields state-of-the-art results and even is competitive with machine learning approaches.","tags":null,"title":"Multiframe Motion Coupling for Video Super Resolution","type":"publication"},{"authors":[],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"2e0c8383197d6debafa57c2a412ec26c","permalink":"https://jonasgeiping.github.io/talk/amm3/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/talk/amm3/","section":"talk","summary":"Visited the ”Workshop: Shape, Images and Optimization”.","tags":[],"title":"3rd Applied Mathematics Symposium Münster","type":"talk"},{"authors":[],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"9bbb7caa69eec99a8397de2ebfe4d6f0","permalink":"https://jonasgeiping.github.io/talk/iccv2017/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/talk/iccv2017/","section":"talk","summary":"Presented the work ”Multiframe Motion Coupling for Video Super Resolution” at 11th International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition. Visited the International Conference on Computer Vision, 2017.","tags":[],"title":"ICCV and EMMCVPR 2017","type":"talk"},{"authors":["Jonas Alexander Geiping"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"41910690aaeb803acebdb4758177035b","permalink":"https://jonasgeiping.github.io/publication/geiping-image-2016/","publishdate":"2023-01-18T23:35:57.449317Z","relpermalink":"/publication/geiping-image-2016/","section":"publication","summary":"Three-dimensional time series data from confocal fluorescence microscopes is a valuable tool in biological research, but the data is distorted by Poisson noise and defocus blur of varying axial extent. We seek to obtain structural information about the develop- ment of neural tissue from these images and define a segmentation by an appropriate thresholding of reconstructed data. We model the data degradation and develop a reconstruction formulation based on variational methods. Due to imprecise knowledge of the blur kernel we extend local sparsity regularization to a local patch and use this prior as additional regularization. We show favorable analytical properties for this approach, implement the resulting algorithm with a primal-dual optimization scheme and test on artificial and real data.","tags":null,"title":"Image Analysis of Neural Tissue Development: Variational Methods for Segmentation and 3D-Reconstruction from Large Pinhole Confocal Fluorescence Microscopy","type":"publication"},{"authors":[],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"e052c52026453c57651d397c8412cd70","permalink":"https://jonasgeiping.github.io/talk/dtu/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/talk/dtu/","section":"talk","summary":"Visited the ”COST Training School on Algebraic Reconstruction Methods in Tomography” and workshop ”HD-Tomo Days”.","tags":[],"title":"Training School and Workshop at DTU University","type":"talk"},{"authors":[],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"becc88d711f702f2f2ceddaa3781c617","permalink":"https://jonasgeiping.github.io/talk/my-talk-name/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/talk/my-talk-name/","section":"talk","summary":"Visited ”Summer School on Inverse Problems” and workshop on ”Variational Methods for Dynamic Inverse Problems and Imaging”","tags":[],"title":"1st Applied Mathematics Symposium Münster","type":"talk"},{"authors":["Jonas Alexander Geiping"],"categories":null,"content":"","date":1409529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409529600,"objectID":"db70ce79017c0fb23c9e4aa9b54f1c81","permalink":"https://jonasgeiping.github.io/publication/geiping-comparison-2014/","publishdate":"2023-01-18T23:35:57.448077Z","relpermalink":"/publication/geiping-comparison-2014/","section":"publication","summary":"","tags":null,"title":"Comparison of Topology-preserving Segmentation Methods and Application to Mitotic Cell Tracking","type":"publication"}]