@article{balestriero_cookbook_2023,
  title = {A {{Cookbook}} of {{Self-Supervised Learning}}},
  author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
  year = {2023},
  month = apr,
  eprint = {2304.12210},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12210},
  url = {http://arxiv.org/abs/2304.12210},
  urldate = {2023-04-25},
  abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  journal = {arxiv:2304.12210[cs]}
}

@article{bansal_cold_2022,
  title = {Cold {{Diffusion}}: {{Inverting Arbitrary Image Transforms Without Noise}}},
  shorttitle = {Cold {{Diffusion}}},
  author = {Bansal, Arpit and Borgnia, Eitan and Chu, Hong-Min and Li, Jie S. and Kazemi, Hamid and Huang, Furong and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  month = aug,
  eprint = {2208.09392},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.09392},
  url = {http://arxiv.org/abs/2208.09392},
  urldate = {2022-10-27},
  abstract = {Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models. The success of these fully deterministic models calls into question the community's understanding of diffusion models, which relies on noise in either gradient Langevin dynamics or variational inference, and paves the way for generalized diffusion models that invert arbitrary processes. Our code is available at https://github.com/arpitbansal297/Cold-Diffusion-Models},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  journal = {arxiv:2208.09392[cs]}
}

@inproceedings{bansal_universal_2023,
  title = {Universal {{Guidance}} for {{Diffusion Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}) {{Workshops}}},
  author = {Bansal, Arpit and Chu, Hong-Min and Schwarzschild, Avi and Sengupta, Soumyadip and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  pages = {843--852},
  url = {https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Bansal\_Universal\_Guidance\_for\_Diffusion\_Models\_CVPRW\_2023\_paper.html},
  urldate = {2023-06-06},
  copyright = {All rights reserved},
  langid = {english}
}

@inproceedings{borgnia_dp-instahide_2021,
  title = {{{DP-InstaHide}}: {{Provably Defusing Poisoning}} and {{Backdoor Attacks}} with {{Differentially Private Data Augmentations}}},
  shorttitle = {{{DP-InstaHide}}},
  booktitle = {{{ICLR}} 2021 {{Workshop}} on {{Security}} and {{Safety}} in {{Machine Learning Systems}}},
  author = {Borgnia, Eitan and Geiping, Jonas and Cherepanova, Valeriia and Fowl, Liam and Gupta, Arjun and Ghiasi, Amin and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  year = {2021},
  month = mar,
  eprint = {2103.02079},
  url = {http://arxiv.org/abs/2103.02079},
  urldate = {2021-03-04},
  abstract = {Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this work, we show that strong data augmentations, such as mixup and random additive noise, nullify poison attacks while enduring only a small accuracy trade-off. To explain these finding, we propose a training method, DP-InstaHide, which combines the mixup regularizer with additive noise. A rigorous analysis of DP-InstaHide shows that mixup does indeed have privacy advantages, and that training with k-way mixup provably yields at least k times stronger DP guarantees than a naive DP mechanism. Because mixup (as opposed to noise) is beneficial to model performance, DP-InstaHide provides a mechanism for achieving stronger empirical performance against poisoning attacks than other known DP methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{borgnia_strong_2021,
  ids = {borgnia_strong_2020},
  title = {Strong {{Data Augmentation Sanitizes Poisoning}} and {{Backdoor Attacks Without}} an {{Accuracy Tradeoff}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Borgnia, Eitan and Cherepanova, Valeriia and Fowl, Liam and Ghiasi, Amin and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Gupta, Arjun},
  year = {2021},
  month = jun,
  eprint = {2011.09527},
  pages = {3855--3859},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414862},
  abstract = {Data poisoning and backdoor attacks manipulate victim models by maliciously modifying training data. In light of this growing threat, a recent survey of industry professionals revealed heightened fear in the private sector regarding data poisoning. Many previous defenses against poisoning either fail in the face of increasingly strong attacks, or they significantly degrade performance. However, we find that strong data augmentations, such as mixup and CutMix, can significantly diminish the threat of poisoning and backdoor attacks without trading off performance. We further verify the effectiveness of this simple defense against adaptive poisoning methods, and we compare to baselines including the popular differentially private SGD (DP-SGD) defense. In the context of backdoors, CutMix greatly mitigates the attack while simultaneously increasing validation accuracy by 9\%.},
  archiveprefix = {arxiv},
  keywords = {Adversarial Attacks,Backdoor Attacks,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Conferences,Data Augmentation,Data models,Data Poisoning,Differential Privacy,Industries,Machine learning,Signal processing,Toxicology,Training data}
}

@inproceedings{chiang_loss_2023,
  title = {Loss {{Landscapes}} Are {{All You Need}}: {{Neural Network Generalization Can Be Explained Without}} the {{Implicit Bias}} of {{Gradient Descent}}},
  shorttitle = {Loss {{Landscapes}} Are {{All You Need}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Chiang, Ping-yeh and Ni, Renkun and Miller, David Yu and Bansal, Arpit and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom},
  year = {2023},
  month = feb,
  url = {https://openreview.net/forum?id=QC10RmRbZy9},
  urldate = {2023-04-06},
  abstract = {It is commonly believed that the implicit regularization of optimizers is needed for neural networks to generalize in the overparameterized regime. In this paper, we observe experimentally that this implicit regularization behavior is \{\textbackslash em generic\}, i.e. it does not depend strongly on the choice of optimizer. We demonstrate this by training neural networks using several gradient-free optimizers, which do not benefit from properties that are often attributed to gradient-based optimizers. This includes a guess-and-check optimizer that generates uniformly random parameter vectors until finding one that happens to achieve perfect train accuracy, and a zeroth-order Pattern Search optimizer that uses no gradient computations. In the low sample and few-shot regimes, where zeroth order optimizers are most computationally tractable, we find that these non-gradient optimizers achieve test accuracy comparable to SGD. The code to reproduce results can be found at https://github.com/Ping-C/optimizer .},
  langid = {english}
}

@inproceedings{chiang_witchcraft_2020,
  ids = {chiang_witchcraft_2019},
  title = {Witchcraft: {{Efficient PGD Attacks}} with {{Random Step Size}}},
  shorttitle = {Witchcraft},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chiang, Ping-Yeh and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Ni, Renkun and Reich, Steven and Shafahi, Ali},
  year = {2020},
  month = may,
  eprint = {1911.07989},
  pages = {3747--3751},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9052930},
  abstract = {State-of-the-art adversarial attacks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more economical, which is important in situations like adversarial training where attacks need to be crafted in real time.},
  archiveprefix = {arxiv},
  keywords = {Adversarial,adversarial attacks,Attack,CIFAR,classical PGD attack,CNN,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,gradient methods,iterative FGSM-based methods,iterative methods,learning (artificial intelligence),neural nets,neural networks,PGD,PGD attacks,projected gradient descent,random step size,Statistics - Machine Learning,stochastic processes,wide iterative stochastic crafting,witchcraft},
  annotation = {ZSCC: 0000002}
}

@inproceedings{chu_panning_2023,
  title = {Panning for {{Gold}} in {{Federated Learning}}: {{Targeted Text Extraction}} under {{Arbitrarily Large-Scale Aggregation}}},
  shorttitle = {Panning for {{Gold}} in {{Federated Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Chu, Hong-Min and Geiping, Jonas and Fowl, Liam H. and Goldblum, Micah and Goldstein, Tom},
  year = {2023},
  month = feb,
  url = {https://openreview.net/forum?id=A9WQaxYsfx},
  urldate = {2023-02-03},
  abstract = {As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase "my credit card number is ...". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.},
  copyright = {All rights reserved},
  langid = {english}
}

@inproceedings{fowl_adversarial_2021,
  title = {Adversarial {{Examples Make Strong Poisons}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fowl, Liam and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojciech and Goldstein, Tom},
  year = {2021},
  volume = {34},
  pages = {30339--30351},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/fe87435d12ef7642af67d9bc82a8b3cd-Abstract.html},
  urldate = {2022-10-31},
  abstract = {The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data.  In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. In fact, adversarial examples with labels re-assigned by the crafting network remain effective for training, suggesting that adversarial examples contain useful semantic content, just with the "wrong" labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.},
  keywords = {⛔ No DOI found,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{fowl_decepticons_2023,
  title = {Decepticons: {{Corrupted Transformers Breach Privacy}} in {{Federated Learning}} for {{Language Models}}},
  shorttitle = {Decepticons},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Fowl, Liam H. and Geiping, Jonas and Reich, Steven and Wen, Yuxin and Czaja, Wojciech and Goldblum, Micah and Goldstein, Tom},
  year = {2023},
  month = feb,
  url = {https://openreview.net/forum?id=r0BrY4BiEXO},
  urldate = {2023-02-03},
  abstract = {A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.},
  copyright = {All rights reserved},
  selected = {true},
  langid = {english}
}

@inproceedings{fowl_preventing_2021,
  title = {Preventing {{Unauthorized Use}} of {{Proprietary Data}}: {{Poisoning}} for {{Secure Dataset Release}}},
  shorttitle = {Preventing {{Unauthorized Use}} of {{Proprietary Data}}},
  booktitle = {{{ICLR}} 2021 {{Workshop}} on {{Security}} and {{Safety}} in {{Machine Learning Systems}}},
  author = {Fowl, Liam and Chiang, Ping-yeh and Goldblum, Micah and Geiping, Jonas and Bansal, Arpit and Czaja, Wojtek and Goldstein, Tom},
  year = {2021},
  month = feb,
  eprint = {2103.02683},
  url = {http://arxiv.org/abs/2103.02683},
  urldate = {2021-03-05},
  abstract = {Large organizations such as social media companies continually release data, for example user images. At the same time, these organizations leverage their massive corpora of released data to train proprietary models that give them an edge over their competitors. These two behaviors can be in conflict as an organization wants to prevent competitors from using their own data to replicate the performance of their proprietary models. We solve this problem by developing a data poisoning method by which publicly released data can be minimally modified to prevent others from train-ing models on it. Moreover, our method can be used in an online fashion so that companies can protect their data in real time as they release it.We demonstrate the success of our approach onImageNet classification and on facial recognition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{fowl_robbing_2021,
  title = {Robbing the {{Fed}}: {{Directly Obtaining Private Data}} in {{Federated Learning}} with {{Modified Models}}},
  shorttitle = {Robbing the {{Fed}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Fowl, Liam and Geiping, Jonas and Czaja, Wojciech and Goldblum, Micah and Goldstein, Tom},
  year = {2021},
  month = sep,
  eprint = {2110.13057},
  url = {https://openreview.net/forum?id=fwzUgo0FM9v},
  urldate = {2022-02-10},
  abstract = {Federated learning has quickly gained popularity with its promises of increased user privacy and efficiency.  Previous works have shown that federated gradient updates contain information that can...},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{gandikota_simple_2022,
  title = {A {{Simple Strategy}} to {{Provable Invariance}} via {{Orbit Mapping}}},
  booktitle = {Asian {{Conference}} on {{Computer Vision}} ({{ACCV}})},
  author = {Gandikota, Kanchana Vaishnavi and Geiping, Jonas and L{\"a}hner, Zorah and Czapli{\'n}ski, Adam and Moeller, Michael},
  year = {2022},
  month = dec,
  eprint = {2209.11916},
  primaryclass = {cs},
  publisher = {{arXiv}},
  address = {{Macau}},
  doi = {10.48550/arXiv.2209.11916},
  url = {http://arxiv.org/abs/2209.11916},
  abstract = {Many applications require robustness, or ideally invariance, of neural networks to certain transformations of input data. Most commonly, this requirement is addressed by training data augmentation, using adversarial training, or defining network architectures that include the desired invariance by design. In this work, we propose a method to make network architectures provably invariant with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. Further, we empirically analyze the properties of different approaches which incorporate invariance via training or architecture, and demonstrate the advantages of our method in terms of robustness and computational efficiency. In particular, we investigate the robustness with respect to rotations of images (which can hold up to discretization artifacts) as well as the provable orientation and scaling invariance of 3D point cloud classification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@phdthesis{geiping_comparison_2014,
  type = {Bachelor {{Thesis}}},
  title = {Comparison of {{Topology-preserving Segmentation Methods}} and {{Application}} to {{Mitotic Cell Tracking}}},
  author = {Geiping, Jonas Alexander},
  year = {2014},
  month = sep,
  address = {{M\"unster}},
  url = {http://wwwmath.uni-muenster.de:8000/num/Arbeitsgruppen/ag\_burger/teaching/Bachelor/BA\%20Geiping.pdf},
  urldate = {2017-01-02},
  school = {Westf\"alischen Wilhelms-Universit\"at M\"unster}
}

@article{geiping_composite_2018,
  title = {Composite {{Optimization}} by {{Nonconvex Majorization-Minimization}}},
  author = {Geiping, Jonas and Moeller, Michael},
  year = {2018},
  month = jan,
  journal = {SIAM Journal on Imaging Sciences},
  pages = {2494--2528},
  doi = {10.1137/18M1171989},
  url = {https://epubs.siam.org/doi/10.1137/18M1171989},
  urldate = {2018-10-26},
  abstract = {The minimization of a nonconvex composite function can model a variety of imaging tasks. A popular class of algorithms for solving such problems are majorization-minimization techniques which iteratively approximate the composite nonconvex function by a majorizing function that is easy to minimize. Most techniques, e.g., gradient descent, utilize convex majorizers in order to guarantee that the majorizer is easy to minimize. In our work we consider a natural class of nonconvex majorizers for these functions, and show that these majorizers are still sufficient for a globally convergent optimization scheme. Numerical results illustrate that by applying this scheme, one can often obtain superior local optima compared to previous majorization-minimization methods, when the nonconvex majorizers are solved to global optimality. Finally, we illustrate the behavior of our algorithm for depth superresolution from raw time-of-flight data.},
  keywords = {{90C26, 90C06, 68U10, 32B20, 65K10, 47J06},Computer Science - Computer Vision and Pattern Recognition,Mathematics - Numerical Analysis,Mathematics - Optimization and Control}
}

@inproceedings{geiping_cramming_2023,
  title = {Cramming: {{Training}} a {{Language Model}} on a Single {{GPU}} in One Day.},
  shorttitle = {Cramming},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = jul,
  pages = {11117--11143},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/geiping23a.html},
  urldate = {2023-08-02},
  selected = {true},
  abstract = {Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting. We provide code to reproduce all experiments at github.com/JonasGeiping/cramming .},
  langid = {english}
}

@article{geiping_darts_2021,
  title = {{{DARTS}} for {{Inverse Problems}}: A {{Study}} on {{Hyperparameter Sensitivity}}},
  shorttitle = {{{DARTS}} for {{Inverse Problems}}},
  author = {Geiping, Jonas and Lukasik, Jovita and Keuper, Margret and Moeller, Michael},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.05647 [cs]},
  eprint = {2108.05647},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.05647},
  urldate = {2021-09-14},
  abstract = {Differentiable architecture search (DARTS) is a widely researched tool for neural architecture search, due to its promising results for image classification. The main benefit of DARTS is the effectiveness achieved through the weight-sharing one-shot paradigm, which allows efficient architecture search. In this work, we investigate DARTS in a systematic case study of inverse problems, which allows us to analyze these potential benefits in a controlled manner. Although we demonstrate that the success of DARTS can be extended from image classification to reconstruction, our experiments yield three fundamental difficulties in the evaluation of DARTS-based methods: First, the results show a large variance in all test cases. Second, the final performance is highly dependent on the hyperparameters of the optimizer. And third, the performance of the weight-sharing architecture used during training does not reflect the final performance of the found architecture well. Thus, we conclude the necessity to 1) report the results of any DARTS-based methods from several runs along with its underlying performance statistics, 2) show the correlation of the training and final architecture performance, and 3) carefully consider if the computational efficiency of DARTS outweighs the costs of hyperparameter optimization and multiple runs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{geiping_fast_2020,
  title = {Fast {{Convex Relaxations}} Using {{Graph Discretizations}}},
  booktitle = {31st {{British Machine Vision Conference}} ({{BMVC}} 2020, {{Oral Presentation}})},
  author = {Geiping, Jonas and Gaede, Fjedor and Bauermeister, Hartmut and Moeller, Michael},
  year = {2020},
  month = sep,
  eprint = {2004.11075},
  address = {{Virtual}},
  url = {https://www.bmvc2020-conference.com/conference/papers/paper\_0694.html},
  urldate = {2020-09-19},
  abstract = {Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Mathematics - Optimization and Control}
}

@inproceedings{geiping_how_2023,
  title = {How {{Much Data Are Augmentations Worth}}? {{An Investigation}} into {{Scaling Laws}}, {{Invariance}}, and {{Implicit Regularization}}},
  shorttitle = {How {{Much Data Are Augmentations Worth}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Geiping, Jonas and Goldblum, Micah and Somepalli, Gowthami and {Shwartz-Ziv}, Ravid and Goldstein, Tom and Wilson, Andrew Gordon},
  year = {2023},
  month = feb,
  url = {https://openreview.net/forum?id=3aQs3MCSexD},
  urldate = {2023-02-03},
  abstract = {Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.},
  copyright = {All rights reserved},
  langid = {english}
}

@mastersthesis{geiping_image_2016,
  title = {Image {{Analysis}} of {{Neural Tissue Development}}: {{Variational Methods}} for {{Segmentation}} and {{3D-Reconstruction}} from Large Pinhole Confocal Fluorescence Microscopy},
  author = {Geiping, Jonas Alexander},
  year = {2016},
  month = sep,
  address = {{M\"unster}},
  abstract = {Three-dimensional time series data from confocal fluorescence microscopes is a valuable tool in biological research, but the data is distorted by Poisson noise and defocus blur of varying axial extent. We seek to obtain structural information about the develop- ment of neural tissue from these images and define a segmentation by an appropriate thresholding of reconstructed data. We model the data degradation and develop a reconstruction formulation based on variational methods. Due to imprecise knowledge of the blur kernel we extend local sparsity regularization to a local patch and use this prior as additional regularization. We show favorable analytical properties for this approach, implement the resulting algorithm with a primal-dual optimization scheme and test on artificial and real data.},
  school = {Westf\"alischen Wilhelms-Universit\"at M\"unster},
  annotation = {00000}
}

@inproceedings{geiping_inverting_2020,
  title = {Inverting {{Gradients}} - {{How}} Easy Is It to Break Privacy in Federated Learning?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Geiping, Jonas and Bauermeister, Hartmut and Dr{\"o}ge, Hannah and Moeller, Michael},
  year = {2020},
  month = dec,
  volume = {33},
  url = {https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html},
  urldate = {2020-12-07},
  langid = {english},
  annotation = {ZSCC: 0000016},
}

@phdthesis{geiping_modern_2021,
  type = {Doctoral {{Thesis}}},
  title = {Modern Optimization Techniques in Computer Vision},
  author = {Geiping, Jonas},
  year = {2021},
  address = {{10.25819/ubsi/9908}},
  url = {https://dspace.ub.uni-siegen.de/handle/ubsi/1897},
  urldate = {2021-05-10},
  abstract = {This thesis presents research into multiple optimization topics in computer vision with a conceptual focus on composite optimization problems such as bilevel optimization. The optimal graph-based discretization of variational problems in minimal partitions, the theoretical analysis of nonconvex composite optimization by nonconvex majorizers, the bilevel problem of learning energy models by nonconvex majorizers, and the machine learning security applications of bilevel optimization in privacy analysis of federated learning and dataset poisoning of general image classification are featured in this cumulative work.},
  langid = {english},
  school = {University of Siegen},
  annotation = {Accepted: 2021-05-10T11:11:26Z}
}

@inproceedings{geiping_multiframe_2018,
  title = {Multiframe {{Motion Coupling}} for {{Video Super Resolution}}},
  booktitle = {Energy {{Minimization Methods}} in {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Geiping, Jonas and Dirks, Hendrik and Cremers, Daniel and Moeller, Michael},
  editor = {Pelillo, Marcello and Hancock, Edwin},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {123--138},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-78199-0 _ 9},
  abstract = {The idea of video super resolution is to use different view points of a single scene to enhance the overall resolution and quality. Classical energy minimization approaches first establish a correspondence of the current frame to all its neighbors in some radius and then use this temporal information for enhancement. In this paper, we propose the first variational super resolution approach that computes several super resolved frames in one batch optimization procedure by incorporating motion information between the high-resolution image frames themselves. As a consequence, the number of motion estimation problems grows linearly in the number of frames, opposed to a quadratic growth of classical methods and temporal consistency is enforced naturally.We use infimal convolution regularization as well as an automatic parameter balancing scheme to automatically determine the reliability of the motion information and reweight the regularization locally. We demonstrate that our approach yields state-of-the-art results and even is competitive with machine learning approaches.},
  isbn = {978-3-319-78199-0},
  langid = {english}
}

@inproceedings{geiping_parametric_2019-1,
  title = {Parametric {{Majorization}} for {{Data-Driven Energy Minimization Methods}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Geiping, Jonas and Moeller, Michael},
  year = {2019},
  eprint = {1908.06209},
  pages = {10262--10273},
  url = {http://openaccess.thecvf.com/content\_ICCV\_2019/html/Geiping\_Parametric\_Majorization\_for\_Data-Driven\_Energy\_Minimization\_Methods\_ICCV\_2019\_paper.html},
  urldate = {2019-11-01},
  abstract = {Energy minimization methods are a classical tool in a multitude of computer vision applications. While they are interpretable and well-studied, their regularity assumptions are difficult to design by hand. Deep learning techniques on the other hand are purely data-driven, often provide excellent results, but are very difficult to constrain to predefined physical or safety-critical models. A possible combination between the two approaches is to design a parametric en- ergy and train the free parameters in such a way that minimizers of the energy correspond to desired solution on a set of training examples. Unfortunately, such formulations typically lead to bi-level optimization problems, on which common optimization algorithms are difficult to scale to modern requirements in data processing and efficiency. In this work, we present a new strategy to optimize these bi-level problems. We investigate surrogate single-level problems that majorize the target problems and can be implemented with existing tools, leading to efficient algorithms without collapse of the energy function. This framework of strategies enables new avenues to the training of parameterized energy minimization models from large data.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {{90C06, 68U10, 68T45, 65K10},Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,G.1.6,G.4,I.4,Mathematics - Optimization and Control}
}

@inproceedings{geiping_stochastic_2021,
  title = {Stochastic {{Training}} Is {{Not Necessary}} for {{Generalization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Geiping, Jonas and Goldblum, Micah and Pope, Phil and Moeller, Michael and Goldstein, Tom},
  year = {2021},
  month = sep,
  eprint = {2109.14119},
  url = {https://openreview.net/forum?id=ZBESeIUB5k},
  urldate = {2022-03-07},
  abstract = {It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks.  In this work, we demonstrate that...},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control}
}

@inproceedings{geiping_what_2021,
  title = {What {{Doesn}}'t {{Kill You Makes You Robust}}(Er): {{Adversarial Training}} against {{Poisons}} and {{Backdoors}}},
  shorttitle = {What {{Doesn}}'t {{Kill You Makes You Robust}}(Er)},
  booktitle = {{{ICLR}} 2021 {{Workshop}} on {{Security}} and {{Safety}} in {{Machine Learning Systems}}},
  author = {Geiping, Jonas and Fowl, Liam and Somepalli, Gowthami and Goldblum, Micah and Moeller, Michael and Goldstein, Tom},
  year = {2021},
  month = feb,
  eprint = {2102.13624},
  url = {http://arxiv.org/abs/2102.13624},
  urldate = {2021-03-01},
  abstract = {Data poisoning is a threat model in which a malicious actor tampers with training data to manipulate outcomes at inference time. A variety of defenses against this threat model have been proposed, but each suffers from at least one of the following flaws: they are easily overcome by adaptive attacks, they severely reduce testing performance, or they cannot generalize to diverse data poisoning threat models. Adversarial training, and its variants, is currently considered the only empirically strong defense against (inference-time) adversarial attacks. In this work, we extend the adversarial training framework to instead defend against (training-time) poisoning and backdoor attacks. Our method desensitizes networks to the effects of poisoning by creating poisons during training and injecting them into training batches. We show that this defense withstands adaptive attacks, generalizes to diverse threat models, and incurs a better performance trade-off than previous defenses.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{geiping_witches_2021,
  ids = {geiping_witches_2020},
  title = {Witches' {{Brew}}: {{Industrial Scale Data Poisoning}} via {{Gradient Matching}}},
  shorttitle = {Witches' {{Brew}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Geiping, Jonas and Fowl, Liam H. and Huang, W. Ronny and Czaja, Wojciech and Taylor, Gavin and Moeller, Michael and Goldstein, Tom},
  year = {2021},
  month = apr,
  eprint = {2009.02276},
  url = {https://openreview.net/forum?id=01olnfLIbD},
  urldate = {2021-01-15},
  abstract = {Data Poisoning attacks modify training data to maliciously control a model trained on such data. Previous poisoning attacks against deep neural networks have been limited in scope and success...},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{goldblum_truth_2020,
  ids = {goldblum_truth_2019},
  title = {Truth or Backpropaganda? {{An}} Empirical Investigation of Deep Learning Theory},
  shorttitle = {Truth or Backpropaganda?},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}} ({{ICLR}} 2020, {{Oral Presentation}})},
  author = {Goldblum, Micah and Geiping, Jonas and Schwarzschild, Avi and Moeller, Michael and Goldstein, Tom},
  year = {2020},
  month = apr,
  eprint = {1910.00359},
  url = {https://iclr.cc/virtual\_2020/poster\_HyxyIgHFvr.html},
  urldate = {2020-09-19},
  abstract = {We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{gorlitz_piecewise_2019-1,
  title = {Piecewise {{Rigid Scene Flow}} with {{Implicit Motion Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {G{\"o}rlitz, Andreas and Geiping, Jonas and Kolb, Andreas},
  year = {2019},
  month = nov,
  pages = {1758--1765},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8968018},
  abstract = {In this paper, we introduce a novel variational approach to estimate the scene flow from RGB-D images. We regularize the ill-conditioned problem of scene flow estimation in a unified framework by enforcing piecewise rigid motion through decomposition into rotational and translational motion parts. Our model crucially regularizes these components by an L0 ``norm'', thereby facilitating implicit motion segmentation in a joint energy minimization problem. Yet, we also show that this energy can be efficiently minimized by a proximal primal-dual algorithm. By implementing this approximate L0 rigid motion regularization, our scene flow estimation approach implicitly segments the observed scene of into regions of nearly constant rigid motion. We evaluate our joint scene flow and segmentation estimation approach on a variety of test scenarios, with and without ground truth data, and demonstrate that we outperform current scene flow techniques.},
  keywords = {constant rigid motion,ill-conditioned problem,image colour analysis,image segmentation,image sequences,implicit motion segmentation estimation approach,joint energy minimization problem,L0 rigid motion regularization,minimisation,motion estimation,observed scene,piecewise rigid motion,piecewise rigid scene flow techniques,proximal primal-dual algorithm,RGB-D images,rotational motion parts,scene flow estimation approach,translational motion parts,variational approach}
}

@inproceedings{huang_metapoison_2020,
  title = {{{MetaPoison}}: {{Practical General-purpose Clean-label Data Poisoning}}},
  shorttitle = {{{MetaPoison}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Huang, W. Ronny and Geiping, Jonas and Fowl, Liam and Taylor, Gavin and Goldstein, Tom},
  year = {2020},
  month = dec,
  volume = {33},
  eprint = {2004.00225},
  address = {{Vancouver, Canada}},
  url = {https://proceedings.neurips.cc//paper\_files/paper/2020/hash/8ce6fc704072e351679ac97d4a985574-Abstract.html},
  urldate = {2020-12-07},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC: 0000007}
}

@article{jain_baseline_2023,
  title = {Baseline {{Defenses}} for {{Adversarial Attacks Against Aligned Language Models}}},
  author = {Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = sep,
  eprint = {2309.00614},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.00614},
  url = {http://arxiv.org/abs/2309.00614},
  urldate = {2023-09-04},
  abstract = {As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more success with filtering and preprocessing than we would expect from other domains, such as vision, providing a first indication that the relative strengths of these defenses may be weighed differently in these domains.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  journal = {arxiv:2309.00614[cs]}
}

@article{jain_bring_2023,
  title = {Bring {{Your Own Data}}! {{Self-Supervised Evaluation}} for {{Large Language Models}}},
  author = {Jain, Neel and Saifullah, Khalid and Wen, Yuxin and Kirchenbauer, John and Shu, Manli and Saha, Aniruddha and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = jun,
  eprint = {2306.13651},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2306.13651},
  urldate = {2023-06-26},
  abstract = {With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arxiv:2306.13651[cs]}
}

@article{jain_how_2022,
  title = {How to {{Do}} a {{Vocab Swap}}? {{A Study}} of {{Embedding Replacement}} for {{Pre-trained Transformers}}},
  shorttitle = {How to {{Do}} a {{Vocab Swap}}?},
  author = {Jain, Neel and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  month = nov,
  url = {https://openreview.net/forum?id=MsjB2ohCJO1},
  urldate = {2023-01-15},
  abstract = {There are a wide range of different tokenizers and vocabularies that have been used to train language models, and training a language model on just one of these can be prohibitively expensive. The ability to swap the vocabulary of a model after it has been trained enables models to be adapted to different tokenizers, and even different languages, without the computational or data cost of from-scratch training. In this paper, we ask when such swaps are possible, and how to perform them effectively? The major challenge of performing a vocab swap is re-learning the parameters of the embedding layer for the vocabulary. We observe that it is possible to re-learn the embedding for a vocabulary using a naive initialization, and we investigate strong initialization strategies that enable learning of new embeddings for swapped vocabularies, even when those vocabularies come from a different source language than the original language model.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{kirchenbauer_reliability_2023,
  title = {On the {{Reliability}} of {{Watermarks}} for {{Large Language Models}}},
  author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Shu, Manli and Saifullah, Khalid and Kong, Kezhi and Fernando, Kasun and Saha, Aniruddha and Goldblum, Micah and Goldstein, Tom},
  year = {2023},
  month = jun,
  eprint = {2306.04634},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.04634},
  url = {http://arxiv.org/abs/2306.04634},
  urldate = {2023-06-08},
  abstract = {Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight our human study, where we investigate the reliability of watermarking when faced with human paraphrasing. We compare watermark-based detection to other detection strategies, finding overall that watermarking is a reliable solution, especially because of its sample complexity - for all attacks we consider, the watermark evidence compounds the more examples are given, and the watermark is eventually detected.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  journal = {arxiv:2306.04634[cs]}
}

@inproceedings{kirchenbauer_watermark_2023,
  title = {A {{Watermark}} for {{Large Language Models}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  year = {2023},
  month = jul,
  pages = {17061--17084},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/kirchenbauer23a.html},
  urldate = {2023-08-02},
  abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
  copyright = {All rights reserved},
  selected = {true},
  langid = {english}
}

@inproceedings{li_augmenters_2023,
  title = {Augmenters at {{SemEval-2023 Task}} 1: {{Enhancing CLIP}} in {{Handling Compositionality}} and {{Ambiguity}} for {{Zero-Shot Visual WSD}} through {{Prompt Augmentation}} and {{Text-To-Image Diffusion}}},
  shorttitle = {Augmenters at {{SemEval-2023 Task}} 1},
  booktitle = {Proceedings of the {{The}} 17th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval-2023}})},
  author = {Li, Jie and Shiue, Yow-Ting and Shih, Yong-Siang and Geiping, Jonas},
  year = {2023},
  month = jul,
  pages = {44--49},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  url = {https://aclanthology.org/2023.semeval-1.5},
  urldate = {2023-07-13},
  abstract = {This paper describes our zero-shot approachesfor the Visual Word Sense Disambiguation(VWSD) Task in English. Our preliminarystudy shows that the simple approach of match-ing candidate images with the phrase usingCLIP suffers from the many-to-many natureof image-text pairs. We find that the CLIP textencoder may have limited abilities in captur-ing the compositionality in natural language.Conversely, the descriptive focus of the phrasevaries from instance to instance. We addressthese issues in our two systems, Augment-CLIPand Stable Diffusion Sampling (SD Sampling).Augment-CLIP augments the text prompt bygenerating sentences that contain the contextphrase with the help of large language mod-els (LLMs). We further explore CLIP modelsin other languages, as the an ambiguous wordmay be translated into an unambiguous one inthe other language. SD Sampling uses text-to-image Stable Diffusion to generate multipleimages from the given phrase, increasing thelikelihood that a subset of images match theone that paired with the text.}
}

@inproceedings{lukasik_differentiable_2023,
  title = {Differentiable {{Architecture Search}}: A {{One-Shot Method}}?},
  shorttitle = {Differentiable {{Architecture Search}}},
  booktitle = {{{AutoML Conference}} 2023},
  author = {Lukasik, Jovita and Geiping, Jonas and Moeller, Michael and Keuper, Margret},
  year = {2023},
  month = aug,
  url = {https://openreview.net/forum?id=LV-5kHj-uV5},
  urldate = {2023-09-10},
  abstract = {Differentiable architecture search (DAS) is a widely researched tool for the design of novel architectures. The main benefit of DAS is the effectiveness achieved through the weight-sharing one-shot paradigm, which allows efficient architecture search. In this work, we investigate DAS in a systematic case study of inverse problems, which allows us to analyze these potential benefits in a controlled manner. We demonstrate that the success of DAS can be extended from image classification to signal reconstruction, in principle. However, our experiments also expose three fundamental difficulties in the evaluation of DAS-based methods in inverse problems: First, the results show a large variance in all test cases. Second, the final performance is strongly dependent on the hyperparameters of the optimizer. And third, the performance of the weight-sharing architecture used during training does not reflect the final performance of the found architecture well. While the results on image reconstruction confirm the potential of the DAS paradigm, they challenge the common understanding of DAS as a one-shot method.},
  langid = {english}
}

@article{ni_k-sam_2022,
  title = {K-{{SAM}}: {{Sharpness-Aware Minimization}} at the {{Speed}} of {{SGD}}},
  shorttitle = {K-{{SAM}}},
  author = {Ni, Renkun and Chiang, Ping-yeh and Geiping, Jonas and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
  year = {2022},
  month = oct,
  eprint = {2210.12864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12864},
  url = {http://arxiv.org/abs/2210.12864},
  urldate = {2022-10-31},
  abstract = {Sharpness-Aware Minimization (SAM) has recently emerged as a robust technique for improving the accuracy of deep neural networks. However, SAM incurs a high computational cost in practice, requiring up to twice as much computation as vanilla SGD. The computational challenge posed by SAM arises because each iteration requires both ascent and descent steps and thus double the gradient computations. To address this challenge, we propose to compute gradients in both stages of SAM on only the top-k samples with highest loss. K-SAM is simple and extremely easy-to-implement while providing significant generalization boosts over vanilla SGD at little to no additional cost.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  journal = {arxiv:2210.12864[cs]}
}

@inproceedings{saifullah_seeing_2023,
  title = {Seeing in {{Words}}: {{Learning}} to {{Classify}} through {{Language Bottlenecks}}},
  shorttitle = {Seeing in {{Words}}},
  booktitle = {{{ICLR TinyPapers}}},
  author = {Saifullah, Khalid and Wen, Yuxin and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom},
  year = {2023},
  month = may,
  url = {https://openreview.net/forum?id=\_QreMdMNIz-},
  urldate = {2023-06-02},
  abstract = {Neural networks for computer vision extract uninterpretable features despite achieving high accuracy on benchmarks. In contrast, humans can explain their predictions using succinct and intuitive descriptions. To incorporate explainability into neural networks, we train a vision model whose feature representations are text. We show that such a model can effectively classify ImageNet images, and we discuss the challenges we encountered when training it.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{sandoval-segura_autoregressive_2022,
  title = {Autoregressive {{Perturbations}} for {{Data Poisoning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{Sandoval-Segura}, Pedro and Singla, Vasu and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Jacobs, David W.},
  year = {2022},
  month = dec,
  url = {https://openreview.net/forum?id=1vusesyN7E},
  urldate = {2022-10-31},
  abstract = {The prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. Data poisoning attacks have been proposed as a bulwark against scraping, as they make data ``unlearnable'' by adding small, imperceptible perturbations. Unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. In this work, we introduce autoregressive (AR) poisoning, a method that can generate poisoned data without access to the broader dataset. The proposed AR perturbations are generic, can be applied across different datasets, and can poison different architectures. Compared to existing unlearnable methods, our AR poisons are more resistant against common defenses such as adversarial training and strong data augmentations. Our analysis further provides insight into what makes an effective data poison.},
  langid = {english}
}

@article{sandoval-segura_jpeg_2023,
  title = {{{JPEG Compressed Images Can Bypass Protections Against AI Editing}}},
  author = {{Sandoval-Segura}, Pedro and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = apr,
  eprint = {2304.02234},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.02234},
  url = {http://arxiv.org/abs/2304.02234},
  urldate = {2023-04-06},
  abstract = {Recently developed text-to-image diffusion models make it easy to edit or create high-quality images. Their ease of use has raised concerns about the potential for malicious editing or deepfake creation. Imperceptible perturbations have been proposed as a means of protecting images from malicious editing by preventing diffusion models from generating realistic images. However, we find that the aforementioned perturbations are not robust to JPEG compression, which poses a major weakness because of the common usage and availability of JPEG. We discuss the importance of robustness for additive imperceptible perturbations and encourage alternative approaches to protect images against editing.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  journal = {arxiv:2304.02234[cs]}
}

@inproceedings{sandoval-segura_poisons_2022,
  title = {Poisons That Are Learned Faster Are More Effective},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {{Sandoval-Segura}, Pedro and Singla, Vasu and Fowl, Liam and Geiping, Jonas and Goldblum, Micah and Jacobs, David and Goldstein, Tom},
  year = {2022},
  month = jun,
  pages = {197--204},
  issn = {2160-7516},
  doi = {10.1109/CVPRW56347.2022.00033},
  abstract = {Imperceptible poisoning attacks on entire datasets have recently been touted as methods for protecting data privacy. However, among a number of defenses preventing the practical use of these techniques, early-stopping stands out as a simple, yet effective defense. To gauge poisons' vulnerability to early-stopping, we benchmark error-minimizing, error-maximizing, and synthetic poisons in terms of peak test accuracy over 100 epochs and make a number of surprising observations. First, we find that poisons that reach a low training loss faster have lower peak test accuracy. Second, we find that a current state-of-the-art error-maximizing poison is 7\texttimes{} less effective when poison training is stopped at epoch 8. Third, we find that stronger, more transferable adversarial attacks do not make stronger poisons. We advocate for evaluating poisons in terms of peak test accuracy.},
  keywords = {Computer vision,Correlation,Data privacy,Perturbation methods,Privacy,Toxicology,Training}
}

@article{sandoval-segura_what_2023,
  title = {What {{Can We Learn}} from {{Unlearnable Datasets}}?},
  author = {{Sandoval-Segura}, Pedro and Singla, Vasu and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom},
  year = {2023},
  month = may,
  eprint = {2305.19254},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.19254},
  url = {http://arxiv.org/abs/2305.19254},
  urldate = {2023-06-03},
  abstract = {In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image privacy is not preserved. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal projection attack which allows learning from unlearnable datasets published in ICML 2021 and ICLR 2023. Our proposed attack is significantly less complex than recently proposed techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  journal = {arxiv:2305.19254[cs]}
}

@article{shu_exploitability_2023,
  title = {On the {{Exploitability}} of {{Instruction Tuning}}},
  author = {Shu, Manli and Wang, Jiongxiao and Zhu, Chen and Geiping, Jonas and Xiao, Chaowei and Goldstein, Tom},
  year = {2023},
  month = jun,
  eprint = {2306.17194},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.17194},
  url = {http://arxiv.org/abs/2306.17194},
  urldate = {2023-07-03},
  abstract = {Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textbackslash textit\{AutoPoison\}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs. Code is available at \textbackslash url\{https://github.com/azshue/AutoPoison\}.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  journal = {arxiv:2306.17194[cs]}
}

@inproceedings{somepalli_diffusion_2023,
  title = {Diffusion {{Art}} or {{Digital Forgery}}? {{Investigating Data Replication}} in {{Diffusion Models}}},
  shorttitle = {Diffusion {{Art}} or {{Digital Forgery}}?},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  pages = {6048--6058},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli\_Diffusion\_Art\_or\_Digital\_Forgery\_Investigating\_Data\_Replication\_in\_Diffusion\_CVPR\_2023\_paper.html},
  urldate = {2023-06-07},
  selected = {true},
  langid = {english}
}

@article{somepalli_understanding_2023,
  title = {Understanding and {{Mitigating Copying}} in {{Diffusion Models}}},
  author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = may,
  eprint = {2305.20086},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.20086},
  url = {http://arxiv.org/abs/2305.20086},
  urldate = {2023-06-01},
  abstract = {Images generated by diffusion models like Stable Diffusion are increasingly widespread. Recent works and even lawsuits have shown that these models are prone to replicating their training data, unbeknownst to the user. In this paper, we first analyze this memorization problem in text-to-image diffusion models. While it is widely believed that duplicated images in the training set are responsible for content replication at inference time, we observe that the text conditioning of the model plays a similarly important role. In fact, we see in our experiments that data replication often does not happen for unconditional models, while it is common in the text-conditional case. Motivated by our findings, we then propose several techniques for reducing data replication at both training and inference time by randomizing and augmenting image captions in the training set.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  journal = {arxiv:2305.20086[cs]}
}

@inproceedings{wen_canary_2023,
  title = {Canary in a {{Coalmine}}: {{Better Membership Inference}} with {{Ensembled Adversarial Queries}}},
  shorttitle = {Canary in a {{Coalmine}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wen, Yuxin and Bansal, Arpit and Kazemi, Hamid and Borgnia, Eitan and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = feb,
  url = {https://openreview.net/forum?id=b7SBTEBFnC},
  urldate = {2023-02-03},
  abstract = {As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings.},
  copyright = {All rights reserved},
  langid = {english}
}

@inproceedings{wen_fishing_2022,
  title = {Fishing for {{User Data}} in {{Large-Batch Federated Learning}} via {{Gradient Magnification}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wen, Yuxin and Geiping, Jonas and Fowl, Liam and Goldblum, Micah and Goldstein, Tom},
  year = {2022},
  month = jun,
  pages = {23668--23684},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/wen22a.html},
  urldate = {2022-09-26},
  abstract = {Federated learning (FL) has rapidly risen in popularity due to its promise of privacy and efficiency. Previous works have exposed privacy vulnerabilities in the FL pipeline by recovering user data from gradient updates. However, existing attacks fail to address realistic settings because they either 1) require toy settings with very small batch sizes, or 2) require unrealistic and conspicuous architecture modifications. We introduce a new strategy that dramatically elevates existing attacks to operate on batches of arbitrarily large size, and without architectural modifications. Our model-agnostic strategy only requires modifications to the model parameters sent to the user, which is a realistic threat model in many scenarios. We demonstrate the strategy in challenging large-scale settings, obtaining high-fidelity data extraction in both cross-device and cross-silo federated learning. Code is available at https://github.com/JonasGeiping/breaching.},
  langid = {english}
}

@article{wen_hard_2023,
  title = {Hard {{Prompts Made Easy}}: {{Gradient-Based Discrete Optimization}} for {{Prompt Tuning}} and {{Discovery}}},
  shorttitle = {Hard {{Prompts Made Easy}}},
  author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = feb,
  url = {https://arxiv.org/abs/2302.03668v1},
  urldate = {2023-02-08},
  abstract = {The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{wen_styx_2023,
  title = {{{STYX}}: {{Adaptive Poisoning Attacks Against Byzantine-Robust Defenses}} in {{Federated Learning}}},
  shorttitle = {{{STYX}}},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wen, Yuxin and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom},
  year = {2023},
  month = jun,
  pages = {1--5},
  doi = {10.1109/ICASSP49357.2023.10096606},
  abstract = {Decentralized training of machine learning models, for instance with federated learning protocols, continues to diffuse from theory toward practical applications and use cases. In federated learning (FL), a central server trains a model collaboratively with a group of users by communicating model updates, without the exchange of private user information. However, these systems can be influenced during training by malicious users who send poisoned updates. Because the training is decentralized and each user controls their own device, these users are free to poison the training protocol. In turn, this has lead to a number of proposals to incorporate aggregation strategies from byzantine-robust learning into the FL paradigm. Byzantine strategies are provably secure for simple model classes, and these robustness properties are often assumed to extend to neural models as well. In this work, we argue that a range of popular robust aggregation strategies, when applied to neural networks, can be trivially circumvented through simple adaptive attacks. We discuss the intuitions behind these adaptive attacks, and show that, despite their simplicity, they provide strong baselines that lead to significant decreases in model performance in FL systems.},
  keywords = {Adaptation models,Adaptive systems,Federated learning,Federated Learning,Model Poisoning,Protocols,Robust Optimization,Security,Threat modeling,Toxicology,Training}
}

@inproceedings{wen_thinking_2022,
  title = {Thinking {{Two Moves Ahead}}: {{Anticipating Other Users Improves Backdoor Attacks}} in {{Federated Learning}}},
  shorttitle = {Thinking {{Two Moves Ahead}}},
  booktitle = {{{AdvML Frontiers}} Workshop at 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wen, Yuxin and Geiping, Jonas and Fowl, Liam and Souri, Hossein and Chellappa, Rama and Goldblum, Micah and Goldstein, Tom},
  year = {2022},
  publisher = {{arXiv}},
  address = {{Baltimore, Maryland, USA}},
  doi = {10.48550/arXiv.2210.09305},
  url = {https://advml-frontier.github.io/pdf/31/CameraReady/backdoor\_fl.pdf},
  urldate = {2022-10-18},
  abstract = {Federated learning is particularly susceptible to model poisoning and backdoor attacks because individual users have direct control over the training data and model updates. At the same time, the attack power of an individual user is limited because their updates are quickly drowned out by those of many other users. Existing attacks do not account for future behaviors of other users, and thus require many sequential updates and their effects are quickly erased. We propose an attack that anticipates and accounts for the entire federated learning pipeline, including behaviors of other clients, and ensures that backdoors are effective quickly and persist even after multiple rounds of community updates. We show that this new attack is effective in realistic scenarios where the attacker only contributes to a small fraction of randomly sampled rounds and demonstrate this attack on image classification, next-word prediction, and sentiment analysis.},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{wen_tree-ring_2023,
  title = {Tree-{{Ring Watermarks}}: {{Fingerprints}} for {{Diffusion Images}} That Are {{Invisible}} and {{Robust}}},
  shorttitle = {Tree-{{Ring Watermarks}}},
  author = {Wen, Yuxin and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = may,
  eprint = {2305.20030},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.20030},
  url = {http://arxiv.org/abs/2305.20030},
  urldate = {2023-06-01},
  abstract = {Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at github.com/YuxinWenRick/tree-ring-watermark.},
  archiveprefix = {arxiv},
  selected = {true},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  journal = {arxiv:2305.20030[cs]}
}
