@article{jain_how_2022,
 abstract = {There are a wide range of different tokenizers and vocabularies that have been used to train language models, and training a language model on just one of these can be prohibitively expensive. The ability to swap the vocabulary of a model after it has been trained enables models to be adapted to different tokenizers, and even different languages, without the computational or data cost of from-scratch training. In this paper, we ask when such swaps are possible, and how to perform them effectively? The major challenge of performing a vocab swap is re-learning the parameters of the embedding layer for the vocabulary. We observe that it is possible to re-learn the embedding for a vocabulary using a naive initialization, and we investigate strong initialization strategies that enable learning of new embeddings for swapped vocabularies, even when those vocabularies come from a different source language than the original language model.},
 author = {Jain, Neel and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
 keywords = {â›” No DOI found},
 langid = {english},
 month = {November},
 shorttitle = {How to Do a Vocab Swap?},
 title = {How to Do a Vocab Swap? A Study of Embedding Replacement for Pre-trained Transformers},
 url = {https://openreview.net/forum?id=MsjB2ohCJO1},
 urldate = {2023-01-15},
 year = {2022}
}

