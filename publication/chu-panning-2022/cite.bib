@inproceedings{chu_panning_2022,
 abstract = {As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase ``my credit card number is ...". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.},
 author = {Chu, Hong-Min and Geiping, Jonas and Fowl, Liam and Goldblum, Micah and Goldstein, Tom},
 booktitle = {NeurIPS ML Safety Workshop},
 copyright = {All rights reserved},
 langid = {english},
 month = {December},
 shorttitle = {Panning for Gold in Federated Learning},
 title = {Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation},
 url = {https://openreview.net/forum?id=Rni_11qO2Z6},
 urldate = {2023-01-15},
 year = {2022}
}

