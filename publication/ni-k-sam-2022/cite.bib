@article{ni_k-sam_2022,
 abstract = {Sharpness-Aware Minimization (SAM) has recently emerged as a robust technique for improving the accuracy of deep neural networks. However, SAM incurs a high computational cost in practice, requiring up to twice as much computation as vanilla SGD. The computational challenge posed by SAM arises because each iteration requires both ascent and descent steps and thus double the gradient computations. To address this challenge, we propose to compute gradients in both stages of SAM on only the top-k samples with highest loss. K-SAM is simple and extremely easy-to-implement while providing significant generalization boosts over vanilla SGD at little to no additional cost.},
 archiveprefix = {arXiv},
 author = {Ni, Renkun and Chiang, Ping-yeh and Geiping, Jonas and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
 doi = {10.48550/arXiv.2210.12864},
 eprint = {2210.12864},
 eprinttype = {arxiv},
 journal = {arxiv:2210.12864[cs]},
 keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
 month = {October},
 primaryclass = {cs},
 publisher = {arXiv},
 shorttitle = {K-SAM},
 title = {K-SAM: Sharpness-Aware Minimization at the Speed of SGD},
 url = {http://arxiv.org/abs/2210.12864},
 urldate = {2022-10-31},
 year = {2022}
}

