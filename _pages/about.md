---
layout: about
title: About Me
permalink: /
subtitle: Research Group Leader <a href='https://institute-tue.ellis.eu/'>ELLIS Institute</a> &  <a href='https://is.mpg.de/'>Max-Planck Institute for Intelligent Systems</a> <br> <a href='https://tuebingen.ai/'>TÃ¼bingen AI Center</a>, Germany

profile:
  align: left
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>TÃ¼bingen, Germany</p>

     <p>ELLIS Institute</p>
     <p>Maria-von-Linden StraÃŸe 2</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

Hi, I'm Jonas. I am a Machine Learning researcher in TÃ¼bingen, where I lead the research group for **safety- & efficiency- aligned learning** (ðŸ¦­). Before this, I've spent time at the Universities of Maryland, Siegen and MÃ¼nster.

I am constantly fascinated by questions of safety and efficiency in modern machine learning. There are a number of fundamental machine learning questions that come up in these topics that we still do not understand well.
On the safety side, I investigate how models can be manipulated through data poisoning, jailbreaks, and adversarial attacks. I'm curious about watermarking for generative models, privacy guarantees in machine learning, and the challenge of defining "safety" in a meaningful technical way. Are there feasible technical solutions that reduce harm?

For efficiency, I study how we can build systems that do more with less, from weight averaging techniques to recursive computation approaches that extend model capabilities. I'm particularly interested in how these systems reason, and whether we can enhance their reasoning abilities while maintaining efficiency. How do we build mechanisms that let these models learn to be intelligent systems?
At the core of my research is this intersection: Can we make models that reason well without sacrificing safety? How do computational constraints affect safety guarantees? Can we design systems where intelligence and safety reinforce each other?

In short:
* Safety, Security and Privacy in Machine Learning
* Efficient Machine Learning (especially in Language Modeling)
* Understanding Reasoning in Intelligent Systems
* Deep Learning as-a-Science


###  Incoming PhD Students:

If you are interested in these topics, feel free to reach out for more information! I'm admitting PhD students on a yearly basis through the following PhD programs:
* [ELLIS PhD program](https://ellis.eu/phd-postdoc)
* [Max Planck & ETH Center for Learning Systems (CLS)](https://learning-systems.org/)
* [International Max Planck Research School for Intelligent Systems (IMPRS-IS)](https://imprs.is.mpg.de/)

For more details, make sure to read the [openings](openings) page carefully.
